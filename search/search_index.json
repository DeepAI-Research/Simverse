{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Simian","text":"<p>Simian is a synthetic data generation tool for creating image caption or video caption pairs. What is novel about Simian is that it focuses on high-quality, specific captioning for features that are difficult to capture in current image and video description models-- for example camera movement, actor intent or physical interactions.</p>"},{"location":"#welcome","title":"Welcome","text":"<p>This documentation is intended to help you understand the structure of the Simian codebase and how to use it to generate synthetic data for your projects. Please select a topic from the sidebar to get started.</p>"},{"location":"#core-use-cases","title":"Core Use Cases","text":"<p>Use Simian data to train image, video and 3D models.</p> <ul> <li> <p>Image and Video Generation: Generate synthetic data for training image and video captioning models. Use Simian to create synthetic data for training image and video captioning models on real data. Mix with real data to add high-level controllability to your model.</p> </li> <li> <p>3D Model Generation: Generate random views or arranged spherical views for training 3D diffusion models.</p> </li> <li> <p>Image and Video Description: Use synthetic data to improve the quality of your captioning, descriptive or contrastive model on real data.</p> </li> <li> <p>3D Object Detection: Generate synthetic data for training 3D object detection models. Use Simian to create synthetic data for training 3D object detection models on real data.</p> </li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Please visit the Getting Started page to learn how to set up your environment and generate synthetic data.</p>"},{"location":"backgrounds/","title":"Backgrounds","text":"<p>Backgrounds are loaded from datasources, such as Poly Haven. The <code>background</code> module is responsible for managing the backgrounds that are loaded into the scene. It handles loading the backgrounds, setting their position, scale and orientation, as well as how materials and textures are handled.</p>"},{"location":"backgrounds/#simian.background.create_photosphere","title":"<code>create_photosphere(hdri_path, combination, scale=10)</code>","text":"<p>Create a photosphere object in the scene.</p> <p>This function creates a UV sphere object in the scene and positions it at (0, 0, 3). It smooths the sphere, inverts its normals, and renames it to \"Photosphere\". It then calls the <code>create_photosphere_material</code> function to create a material for the photosphere using the environment texture as emission.</p> <p>Parameters:</p> Name Type Description Default <code>hdri_path</code> <code>str</code> <p>The base directory for storing background images.</p> required <code>combination</code> <code>Dict</code> <p>The combination dictionary containing background information.</p> required <p>Returns:</p> Type Description <code>Object</code> <p>bpy.types.Object: The created photosphere object.</p> Source code in <code>simian/background.py</code> <pre><code>def create_photosphere(\n    hdri_path: str, combination: Dict, scale: float = 10\n) -&gt; bpy.types.Object:\n    \"\"\"\n    Create a photosphere object in the scene.\n\n    This function creates a UV sphere object in the scene and positions it at (0, 0, 3).\n    It smooths the sphere, inverts its normals, and renames it to \"Photosphere\". It then\n    calls the `create_photosphere_material` function to create a material for the photosphere\n    using the environment texture as emission.\n\n    Args:\n        hdri_path (str): The base directory for storing background images.\n        combination (Dict): The combination dictionary containing background information.\n\n    Returns:\n        bpy.types.Object: The created photosphere object.\n    \"\"\"\n    bpy.ops.mesh.primitive_uv_sphere_add(\n        segments=64, ring_count=32, radius=scale, location=(0, 0, 3)\n    )\n\n    bpy.ops.object.shade_smooth()\n\n    # invert the UV sphere normals\n    bpy.ops.object.mode_set(mode=\"EDIT\")\n    bpy.ops.mesh.select_all(action=\"SELECT\")\n    bpy.ops.mesh.flip_normals()\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n\n    sphere = bpy.context.object\n    sphere.name = \"Photosphere\"\n    sphere.data.name = \"PhotosphereMesh\"\n    create_photosphere_material(hdri_path, combination, sphere)\n    return sphere\n</code></pre>"},{"location":"backgrounds/#simian.background.create_photosphere_material","title":"<code>create_photosphere_material(hdri_path, combination, sphere)</code>","text":"<p>Create a material for the photosphere object using the environment texture as emission.</p> <p>This function creates a new material for the provided photosphere object. It sets up the material nodes to use the environment texture as emission and assigns the material to the photosphere object.</p> <p>Parameters:</p> Name Type Description Default <code>hdri_path</code> <code>str</code> <p>The base directory for storing background images.</p> required <code>combination</code> <code>Dict</code> <p>The combination dictionary containing background information.</p> required <code>sphere</code> <code>Object</code> <p>The photosphere object to assign the material to.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/background.py</code> <pre><code>def create_photosphere_material(\n    hdri_path: str, combination: Dict, sphere: bpy.types.Object\n) -&gt; None:\n    \"\"\"\n    Create a material for the photosphere object using the environment texture as emission.\n\n    This function creates a new material for the provided photosphere object. It sets up\n    the material nodes to use the environment texture as emission and assigns the material\n    to the photosphere object.\n\n    Args:\n        hdri_path (str): The base directory for storing background images.\n        combination (Dict): The combination dictionary containing background information.\n        sphere (bpy.types.Object): The photosphere object to assign the material to.\n\n    Returns:\n        None\n    \"\"\"\n    # Create a new material\n    mat = bpy.data.materials.new(name=\"PhotosphereMaterial\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    nodes.clear()\n\n    # Create and connect the nodes\n    emission = nodes.new(type=\"ShaderNodeEmission\")\n    env_tex = nodes.new(type=\"ShaderNodeTexEnvironment\")\n    env_tex.image = bpy.data.images.load(get_hdri_path(hdri_path, combination))\n    mat.node_tree.links.new(env_tex.outputs[\"Color\"], emission.inputs[\"Color\"])\n    output = nodes.new(type=\"ShaderNodeOutputMaterial\")\n    mat.node_tree.links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n    # Assign material to the sphere\n    if sphere.data.materials:\n        sphere.data.materials[0] = mat\n    else:\n        sphere.data.materials.append(mat)\n</code></pre>"},{"location":"backgrounds/#simian.background.get_background","title":"<code>get_background(hdri_path, combination)</code>","text":"<p>Download the background HDR image if it doesn't exist locally.</p> <p>This function checks if the background HDR image specified in the combination dictionary exists locally. If it doesn't exist, it downloads the image from the provided URL and saves it to the local file path.</p> <p>Parameters:</p> Name Type Description Default <code>hdri_path</code> <code>str</code> <p>The base directory for storing background images.</p> required <code>combination</code> <code>Dict</code> <p>The combination dictionary containing background information.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/background.py</code> <pre><code>def get_background(hdri_path: str, combination: Dict) -&gt; None:\n    \"\"\"\n    Download the background HDR image if it doesn't exist locally.\n\n    This function checks if the background HDR image specified in the combination dictionary\n    exists locally. If it doesn't exist, it downloads the image from the provided URL and\n    saves it to the local file path.\n\n    Args:\n        hdri_path (str): The base directory for storing background images.\n        combination (Dict): The combination dictionary containing background information.\n\n    Returns:\n        None\n    \"\"\"\n    hdri_path = get_hdri_path(hdri_path, combination)\n\n    background = combination[\"background\"]\n    background_url = background[\"url\"]\n\n    # make sure each folder in the path exists\n    os.makedirs(os.path.dirname(hdri_path), exist_ok=True)\n\n    if not os.path.exists(hdri_path):\n        # logger.info(f\"Downloading {background_url} to {hdri_path}\")\n        response = requests.get(background_url)\n        with open(hdri_path, \"wb\") as file:\n            file.write(response.content)\n</code></pre>"},{"location":"backgrounds/#simian.background.get_hdri_path","title":"<code>get_hdri_path(hdri_path, combination)</code>","text":"<p>Get the local file path for the background HDR image.</p> <p>Parameters:</p> Name Type Description Default <code>hdri_path</code> <code>str</code> <p>The base directory for storing background images.</p> required <code>combination</code> <code>Dict</code> <p>The combination dictionary containing background</p> required <p>Returns:     str: The local file path for the background HDR image.</p> Source code in <code>simian/background.py</code> <pre><code>def get_hdri_path(hdri_path: str, combination: Dict) -&gt; str:\n    \"\"\"\n    Get the local file path for the background HDR image.\n\n    Args:\n        hdri_path (str): The base directory for storing background images.\n        combination (Dict): The combination dictionary containing background\n    Returns:\n        str: The local file path for the background HDR image.\n    \"\"\"\n    background = combination[\"background\"]\n    background_id = background[\"id\"]\n    background_from = background[\"from\"]\n    hdri_path = f\"{hdri_path}/{background_from}/{background_id}.hdr\"\n\n    return hdri_path\n</code></pre>"},{"location":"backgrounds/#simian.background.set_background","title":"<code>set_background(hdri_path, combination)</code>","text":"<p>Set the background HDR image of the scene.</p> <p>This function sets the background HDR image of the scene using the provided combination dictionary. It ensures that the world nodes are used and creates the necessary nodes (Environment Texture, Background, and World Output) if they don't exist. It then loads the HDR image, connects the nodes, and enables the world background in the render settings.</p> <p>Parameters:</p> Name Type Description Default <code>hdri_path</code> <code>str</code> <p>The base directory for storing background images.</p> required <code>combination</code> <code>Dict</code> <p>The combination dictionary containing background information.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/background.py</code> <pre><code>def set_background(hdri_path: str, combination: Dict) -&gt; None:\n    \"\"\"\n    Set the background HDR image of the scene.\n\n    This function sets the background HDR image of the scene using the provided combination\n    dictionary. It ensures that the world nodes are used and creates the necessary nodes\n    (Environment Texture, Background, and World Output) if they don't exist. It then loads\n    the HDR image, connects the nodes, and enables the world background in the render settings.\n\n    Args:\n        hdri_path (str): The base directory for storing background images.\n        combination (Dict): The combination dictionary containing background information.\n\n    Returns:\n        None\n    \"\"\"\n    get_background(hdri_path, combination)\n    hdri_path = get_hdri_path(hdri_path, combination)\n\n    # Check if the scene has a world, and create one if it doesn't\n    if bpy.context.scene.world is None:\n        bpy.context.scene.world = bpy.data.worlds.new(\"World\")\n\n    # Ensure world nodes are used\n    bpy.context.scene.world.use_nodes = True\n    tree = bpy.context.scene.world.node_tree\n\n    # Clear existing nodes\n    tree.nodes.clear()\n\n    # Create the Environment Texture node\n    env_tex_node = tree.nodes.new(type=\"ShaderNodeTexEnvironment\")\n    env_tex_node.location = (-300, 0)\n\n    # Load the HDR image\n    env_tex_node.image = bpy.data.images.load(hdri_path)\n\n    # Create the Background node\n    background_node = tree.nodes.new(type=\"ShaderNodeBackground\")\n    background_node.location = (0, 0)\n\n    # Connect the Environment Texture node to the Background node\n    tree.links.new(env_tex_node.outputs[\"Color\"], background_node.inputs[\"Color\"])\n\n    # Create the World Output node\n    output_node = tree.nodes.new(type=\"ShaderNodeOutputWorld\")\n    output_node.location = (300, 0)\n\n    # Connect the Background node to the World Output\n    tree.links.new(background_node.outputs[\"Background\"], output_node.inputs[\"Surface\"])\n\n    # Enable the world background in the render settings\n    bpy.context.scene.render.film_transparent = False\n</code></pre>"},{"location":"batch_rendering/","title":"Batch Rendering","text":"<p>Local batch processing is handled by the <code>batch</code> module. This module is responsible for generating videos in bulk. It calls the <code>render</code> module to generate videos, iterating over all combinations from the supplied start index to the end index.</p>"},{"location":"batch_rendering/#simian.batch.parse_args","title":"<code>parse_args(args_list=None)</code>","text":"<p>Parse the command-line arguments for the rendering process.</p> <p>Parameters:</p> Name Type Description Default <code>args_list</code> <code>Optional[List[str]]</code> <p>A list of command-line arguments. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Namespace</code> <p>argparse.Namespace: The parsed command-line arguments.</p> Source code in <code>simian/batch.py</code> <pre><code>def parse_args(args_list = None) -&gt; argparse.Namespace:\n    \"\"\"\n    Parse the command-line arguments for the rendering process.\n\n    Args:\n        args_list (Optional[List[str]]): A list of command-line arguments. Defaults to None.\n\n    Returns:\n        argparse.Namespace: The parsed command-line arguments.\n    \"\"\"\n\n    parser = argparse.ArgumentParser(\n        description=\"Automate the rendering of objects using Blender.\"\n    )\n    parser.add_argument(\n        \"--processes\",\n        type=int,\n        default=None,\n        help=\"Number of processes to use for multiprocessing. Defaults to three times the number of CPU cores.\",\n    )\n    parser.add_argument(\n        \"--render_timeout\",\n        type=int,\n        default=3000,\n        help=\"Maximum time in seconds for a single rendering process. Defaults to 3000.\",\n    )\n    parser.add_argument(\n        \"--width\",\n        type=int,\n        default=1920,\n        help=\"Width of the rendering in pixels. Defaults to 1920.\",\n    )\n    parser.add_argument(\n        \"--height\",\n        type=int,\n        default=1080,\n        help=\"Height of the rendering in pixels. Defaults to 1080.\",\n    )\n    parser.add_argument(\n        \"--start_index\",\n        type=int,\n        default=0,\n        help=\"Starting index for rendering from the combinations DataFrame. Defaults to 0.\",\n    )\n    parser.add_argument(\n        \"--end_index\",\n        type=int,\n        default=-1,\n        help=\"Ending index for rendering from the combinations DataFrame. Defaults to -1.\",\n    )\n    parser.add_argument(\n        \"--start_frame\",\n        type=int,\n        default=1,\n        help=\"Starting frame number for the animation. Defaults to 1.\",\n    )\n    parser.add_argument(\n        \"--end_frame\",\n        type=int,\n        default=65,\n        help=\"Ending frame number for the animation. Defaults to 65.\",\n    )\n    parser.add_argument(\n        \"--images\",\n        action=\"store_true\",\n        help=\"Generate images instead of videos.\",\n    )\n    parser.add_argument(\n        \"--blend\",\n        type=str,\n        default=None,\n        help=\"Path to the user-specified Blender file to use as the base scene.\",\n        required=False,\n    )\n    parser.add_argument(\n        \"--animation_length\",\n        type=int,\n        default=100,\n        help=\"Percentage animation length. Defaults to 100%.\",\n        required=False\n    )\n\n    if args_list is None:\n        args = parser.parse_args()\n    else:\n        args = parser.parse_args(args_list)\n\n    return args\n</code></pre>"},{"location":"batch_rendering/#simian.batch.prompt_based_rendering","title":"<code>prompt_based_rendering()</code>","text":"<p>Perform prompt-based rendering using the Gemini API and ChromaDB.</p> Source code in <code>simian/batch.py</code> <pre><code>def prompt_based_rendering():\n    \"\"\"\n    Perform prompt-based rendering using the Gemini API and ChromaDB.\n    \"\"\"\n\n    import random\n    from chromadb.utils import embedding_functions\n    from sentence_transformers import SentenceTransformer\n\n    chroma_client = initialize_chroma_db(reset_hdri=False, reset_textures=False)\n\n    # Create or get collections for each data type\n    object_collection = chroma_client.get_or_create_collection(name=\"object_captions\")\n    hdri_collection = chroma_client.get_or_create_collection(name=\"hdri_backgrounds\")\n    texture_collection = chroma_client.get_or_create_collection(name=\"textures\")\n\n    prompt = input(\"Enter your prompt (or 'quit' to exit): \")\n\n    # Generate Gemini\n    model = setup_gemini()\n    objects_background_ground_prompt = generate_gemini(model, OBJECTS_PROMPT, prompt)\n    objects_background_ground_list = json.loads(objects_background_ground_prompt)\n\n    # split array, background and ground are last two elements\n    objects_prompt = objects_background_ground_list[:-2]\n    background_prompt = objects_background_ground_list[-2]\n    ground_prompt = objects_background_ground_list[-1]\n\n    object_ids = []\n    for i, obj in enumerate(objects_prompt):\n        object_options =  query_collection(obj, object_collection, n_results=2)\n        object_ids.append({object_options[\"ids\"][0][0]: obj})\n\n    background_query = query_collection(background_prompt, hdri_collection, n_results=2)\n    background_id = background_query[\"ids\"][0][0]\n    background_data = background_query[\"metadatas\"][0][0]\n\n    formatted_background = {\n        \"background\": {\n            \"name\": background_data['name'],\n            \"url\": background_data['url'],\n            \"id\": background_id,\n            \"from\": \"hdri_data\"\n        }\n    }\n\n    ground_texture_query = query_collection(ground_prompt, texture_collection, n_results=2)\n    ground_data = ground_texture_query[\"metadatas\"][0][0]\n\n    formatted_stage = {\n        \"stage\": {\n            \"material\": {\n                \"name\": ground_data['name'],\n                \"maps\": ground_data['maps']\n            },\n            \"uv_scale\": [random.uniform(0.8, 1.2), random.uniform(0.8, 1.2)],\n            \"uv_rotation\": random.uniform(0, 360)\n        }\n    }\n\n    prompt += str(object_ids)\n\n    objects_json_prompt = generate_gemini(model, OBJECTS_JSON_PROMPT, prompt + str(object_ids))\n    objects_parse = parse_gemini_json(objects_json_prompt)\n\n    camera_prompt = generate_gemini(model, CAMERA_PROMPT, prompt)\n    camera_parse = parse_gemini_json(camera_prompt)\n\n    # Combine all pieces into the final structure\n    final_structure = {\n        \"index\": 0,\n        \"objects\": objects_parse.get(\"objects\"),\n        \"background\": formatted_background[\"background\"],\n        \"orientation\": camera_parse.get(\"orientation\", {}),\n        \"framing\": camera_parse.get(\"framing\", {}),\n        \"animation\": camera_parse.get(\"animation\", {}),\n        \"stage\": formatted_stage[\"stage\"],\n        \"postprocessing\": camera_parse.get(\"postprocessing\", {})\n    }\n\n    if not should_apply_movement(final_structure[\"objects\"]):\n        final_structure[\"no_movement\"] = True\n\n    updated_combination = calculate_transformed_positions(final_structure)\n    write_combinations_json(updated_combination)\n\n    # Get the directory of the current script\n    current_dir = os.path.dirname(os.path.realpath(__file__))\n\n    # Set default rendering parameters\n    width = 1024\n    height = 576\n    start_frame = 1\n    end_frame = 65\n\n    render_objects(\n        processes=None,\n        render_timeout=3000,\n        width=width,\n        height=height,\n        start_index=0, \n        end_index=1,\n        start_frame=start_frame,\n        end_frame=end_frame,\n        images=False,  \n        blend_file=None,\n        animation_length=100 \n    )\n\n    return updated_combination\n</code></pre>"},{"location":"batch_rendering/#simian.batch.render_objects","title":"<code>render_objects(processes=None, render_timeout=3000, width=1920, height=1080, start_index=0, end_index=-1, start_frame=1, end_frame=65, images=False, blend_file=None, animation_length=100)</code>","text":"<p>Automates the rendering of objects using Blender based on predefined combinations.</p> <p>This function orchestrates the rendering of multiple objects within a specified range from the combinations DataFrame. It allows for configuration of rendering dimensions, use of specific GPU devices, and selection of frames for animation sequences.</p> <p>Parameters:</p> Name Type Description Default <code>processes</code> <code>Optional[int]</code> <p>Number of processes to use for multiprocessing.</p> <code>None</code> <code>render_timeout</code> <code>int</code> <p>Maximum time in seconds for a single rendering process.</p> <code>3000</code> <code>width</code> <code>int</code> <p>Width of the rendering in pixels.</p> <code>1920</code> <code>height</code> <code>int</code> <p>Height of the rendering in pixels.</p> <code>1080</code> <code>start_index</code> <code>int</code> <p>Starting index for rendering from the combinations DataFrame.</p> <code>0</code> <code>end_index</code> <code>int</code> <p>Ending index for rendering from the combinations DataFrame.</p> <code>-1</code> <code>start_frame</code> <code>int</code> <p>Starting frame number for the animation.</p> <code>1</code> <code>end_frame</code> <code>int</code> <p>Ending frame number for the animation.</p> <code>65</code> <code>images</code> <code>bool</code> <p>Generate images instead of videos.</p> <code>False</code> <code>blend_file</code> <code>Optional[str]</code> <p>Path to the user-specified Blender file to use as the base scene.</p> <code>None</code> <code>animation_length</code> <code>int</code> <p>Percentage animation length.</p> <code>100</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the operating system is not supported.</p> <code>FileNotFoundError</code> <p>If Blender is not found at the specified path.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/batch.py</code> <pre><code>def render_objects(\n    processes: Optional[int] = None,\n    render_timeout: int = 3000,\n    width: int = 1920,\n    height: int = 1080,\n    start_index: int = 0,\n    end_index: int = -1,\n    start_frame: int = 1,\n    end_frame: int = 65,\n    images: bool = False,\n    blend_file: Optional[str] = None,\n    animation_length: int = 100\n) -&gt; None:\n    \"\"\"\n    Automates the rendering of objects using Blender based on predefined combinations.\n\n    This function orchestrates the rendering of multiple objects within a specified range\n    from the combinations DataFrame. It allows for configuration of rendering dimensions,\n    use of specific GPU devices, and selection of frames for animation sequences.\n\n    Args:\n        processes (Optional[int]): Number of processes to use for multiprocessing.\n        Defaults to three times the number of CPU cores.\n        render_timeout (int): Maximum time in seconds for a single rendering process.\n        width (int): Width of the rendering in pixels.\n        height (int): Height of the rendering in pixels.\n        start_index (int): Starting index for rendering from the combinations DataFrame.\n        end_index (int): Ending index for rendering from the combinations DataFrame.\n        start_frame (int): Starting frame number for the animation.\n        end_frame (int): Ending frame number for the animation.\n        images (bool): Generate images instead of videos.\n        blend_file (Optional[str]): Path to the user-specified Blender file to use as the base scene.\n        animation_length (int): Percentage animation length.\n\n    Raises:\n        NotImplementedError: If the operating system is not supported.\n        FileNotFoundError: If Blender is not found at the specified path.\n\n    Returns:\n        None\n    \"\"\"\n    # Set the number of processes to three times the number of CPU cores if not specified.\n    if processes is None:\n        processes = multiprocessing.cpu_count() * 3\n\n    scripts_dir = os.path.dirname(os.path.realpath(__file__))\n    target_directory = os.path.join(scripts_dir, \"../\", \"renders\")\n    hdri_path = os.path.join(scripts_dir, \"../\", \"backgrounds\")\n\n    # make sure renders directory exists\n    os.makedirs(target_directory, exist_ok=True)\n\n    if end_index == -1:\n\n        # get the length of combinations.json\n        with open(os.path.join(scripts_dir, \"../\", \"combinations.json\"), \"r\") as file:\n            data = json.load(file)\n            combinations_data = data[\"combinations\"]\n            num_combinations = len(combinations_data)\n\n        end_index = num_combinations\n\n    # Loop over each combination index to set up and run the rendering process.\n    for i in range(start_index, end_index):\n        if images:\n            args = f\"--width {width} --height {height} --combination_index {i} --start_frame {start_frame} --end_frame {end_frame} --output_dir {target_directory} --hdri_path {hdri_path} --animation_length {animation_length} --images\"\n        else:\n            args = f\"--width {width} --height {height} --combination_index {i} --start_frame {start_frame} --end_frame {end_frame} --output_dir {target_directory} --hdri_path {hdri_path} --animation_length {animation_length}\"\n\n        if blend_file:\n            args += f\" --blend {blend_file}\"\n\n        command = f\"{sys.executable} -m simian.render -- {args}\"\n        subprocess.run([\"bash\", \"-c\", command], timeout=render_timeout, check=False)\n</code></pre>"},{"location":"batch_rendering/#simian.batch.select_mode","title":"<code>select_mode()</code>","text":"<p>Prompt the user to select the rendering mode (Prompt Mode or Batch Mode).</p> Source code in <code>simian/batch.py</code> <pre><code>def select_mode():\n    \"\"\"\n    Prompt the user to select the rendering mode (Prompt Mode or Batch Mode).\n    \"\"\"\n\n    custom_style = Style([\n        ('question', 'fg:#FF9D00 bold'), \n        ('pointer', 'fg:#FF9D00 bold'),  \n        ('highlighted', 'fg:#00FFFF bold'),\n        ('default', 'fg:#FFFFFF'),\n    ])\n\n    options = [\n        Choice(\"Prompt Mode\", value=\"Prompt Mode\"),\n        Choice(\"Batch Mode\", value=\"Batch Mode\")\n    ]\n\n    selected = select(\n        \"Select Mode\",\n        choices=options,\n        use_indicator=True,\n        style=custom_style,\n        instruction='Use \u2191 and \u2193 to navigate, Enter to select',\n        qmark=\"\", \n        pointer=\"\u25b6\", \n    ).ask()\n\n    console.print(f\"\\nEntering {selected}\", style=\"bold green\")\n\n    return selected\n</code></pre>"},{"location":"batch_rendering/#simian.batch.should_apply_movement","title":"<code>should_apply_movement(all_objects)</code>","text":"<p>Check if any object in the scene has movement defined.</p> <p>Parameters:</p> Name Type Description Default <code>all_objects</code> <code>list</code> <p>List of object dictionaries.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if any object has movement, False otherwise.</p> Source code in <code>simian/batch.py</code> <pre><code>def should_apply_movement(all_objects):\n    \"\"\"\n    Check if any object in the scene has movement defined.\n\n    Args:\n        all_objects (list): List of object dictionaries.\n\n    Returns:\n        bool: True if any object has movement, False otherwise.\n    \"\"\"\n    return any('movement' in obj for obj in all_objects)\n</code></pre>"},{"location":"batch_rendering/#simian.batch.write_combinations_json","title":"<code>write_combinations_json(combination, output_file='combinations.json')</code>","text":"<p>Write the combination data to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>dict</code> <p>The combination data to write.</p> required <code>output_file</code> <code>str</code> <p>The name of the output JSON file. Defaults to \"combinations.json\".</p> <code>'combinations.json'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>simian/batch.py</code> <pre><code>def write_combinations_json(combination: Dict[str, Any], output_file: str = \"combinations.json\"):\n    \"\"\"\n    Write the combination data to a JSON file.\n\n    Args:\n        combination (dict): The combination data to write.\n        output_file (str): The name of the output JSON file. Defaults to \"combinations.json\".\n\n    Returns:\n        None\n    \"\"\"\n    # Prepare the data structure\n    data = {\n        \"seed\": 0,  # You might want to make this dynamic\n        \"count\": 1,  # Since we're only writing one combination\n        \"combinations\": [combination]\n    }\n\n    # Get the directory of the current script\n    current_dir = os.path.dirname(os.path.realpath(__file__))\n\n    # Construct the full path for the output file\n    output_path = os.path.join(current_dir, \"..\", output_file)\n\n    # Write the data to the JSON file\n    with open(output_path, 'w') as f:\n        json.dump(data, f, indent=4)\n</code></pre>"},{"location":"cameras/","title":"Cameras","text":"<p>The <code>camera</code> module is responsible for managing the camera in Blender. It handles setting up the camera, setting the camera position, orientation, and field of view.</p>"},{"location":"cameras/#simian.camera.compute_camera_distance","title":"<code>compute_camera_distance(points, fov_deg)</code>","text":"<p>Calculate the camera distance required to frame the bounding sphere of the points.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The points to frame.</p> required <code>fov_deg</code> <code>float</code> <p>The field of view in degrees.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>The camera distance, centroid of the points, and the radius of the bounding sphere.</p> Source code in <code>simian/camera.py</code> <pre><code>def compute_camera_distance(points, fov_deg):\n    \"\"\"\n    Calculate the camera distance required to frame the bounding sphere of the points.\n\n    Args:\n        points (np.ndarray): The points to frame.\n        fov_deg (float): The field of view in degrees.\n\n    Returns:\n        tuple: The camera distance, centroid of the points, and the radius of the bounding sphere.\n\n    \"\"\"\n    # Calculate the center of the bounding sphere (use the centroid for simplicity)\n    centroid = np.mean(points, axis=0)\n    # Calculate the radius as the max distance from the centroid to any point\n    radius = np.max(np.linalg.norm(points - centroid, axis=1))\n    # Calculate the camera distance using the radius and the field of view\n    fov_rad = math.radians(fov_deg)\n    distance = radius / math.tan(fov_rad / 2)\n    return distance, centroid, radius\n</code></pre>"},{"location":"cameras/#simian.camera.create_camera_rig","title":"<code>create_camera_rig()</code>","text":"<p>Creates a camera rig consisting of multiple objects in Blender.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Object</code> <p>A dictionary containing the created objects: - camera_animation_root: The root object of the camera animation hierarchy. - camera_orientation_pivot_yaw: The yaw pivot object for camera orientation. - camera_orientation_pivot_pitch: The pitch pivot object for camera orientation. - camera_framing_pivot: The pivot object for camera framing. - camera_animation_pivot: The pivot object for camera animation. - camera_object: The camera object. - camera: The camera data.</p> Source code in <code>simian/camera.py</code> <pre><code>def create_camera_rig() -&gt; bpy.types.Object:\n    \"\"\"\n    Creates a camera rig consisting of multiple objects in Blender.\n\n    Returns:\n        dict: A dictionary containing the created objects:\n            - camera_animation_root: The root object of the camera animation hierarchy.\n            - camera_orientation_pivot_yaw: The yaw pivot object for camera orientation.\n            - camera_orientation_pivot_pitch: The pitch pivot object for camera orientation.\n            - camera_framing_pivot: The pivot object for camera framing.\n            - camera_animation_pivot: The pivot object for camera animation.\n            - camera_object: The camera object.\n            - camera: The camera data.\n    \"\"\"\n    camera_animation_root = bpy.data.objects.new(\"CameraAnimationRoot\", None)\n    bpy.context.scene.collection.objects.link(camera_animation_root)\n\n    camera_orientation_pivot_yaw = bpy.data.objects.new(\n        \"CameraOrientationPivotYaw\", None\n    )\n    camera_orientation_pivot_yaw.parent = camera_animation_root\n    bpy.context.scene.collection.objects.link(camera_orientation_pivot_yaw)\n\n    camera_orientation_pivot_pitch = bpy.data.objects.new(\n        \"CameraOrientationPivotPitch\", None\n    )\n    camera_orientation_pivot_pitch.parent = camera_orientation_pivot_yaw\n    bpy.context.scene.collection.objects.link(camera_orientation_pivot_pitch)\n\n    camera_framing_pivot = bpy.data.objects.new(\"CameraFramingPivot\", None)\n    camera_framing_pivot.parent = camera_orientation_pivot_pitch\n    bpy.context.scene.collection.objects.link(camera_framing_pivot)\n\n    camera_animation_pivot = bpy.data.objects.new(\"CameraAnimationPivot\", None)\n    camera_animation_pivot.parent = camera_framing_pivot\n    bpy.context.scene.collection.objects.link(camera_animation_pivot)\n\n    camera = bpy.data.cameras.new(\"Camera\")\n    camera_object = bpy.data.objects.new(\"Camera\", camera)\n\n    # Rotate the Camera 90\u00ba\n    camera_object.delta_rotation_euler = [1.5708, 0, 1.5708]\n    camera_object.data.lens_unit = \"FOV\"\n\n    camera_object.parent = camera_animation_pivot\n    bpy.context.scene.collection.objects.link(camera_object)\n\n    bpy.context.scene.camera = camera_object\n\n    return {\n        \"camera_animation_root\": camera_animation_root,\n        \"camera_orientation_pivot_yaw\": camera_orientation_pivot_yaw,\n        \"camera_orientation_pivot_pitch\": camera_orientation_pivot_pitch,\n        \"camera_framing_pivot\": camera_framing_pivot,\n        \"camera_animation_pivot\": camera_animation_pivot,\n        \"camera_object\": camera_object,\n        \"camera\": camera,\n    }\n</code></pre>"},{"location":"cameras/#simian.camera.perspective_project","title":"<code>perspective_project(points, camera_distance, fov_deg, aspect_ratio=1.0)</code>","text":"<p>Project points onto a 2D plane using a perspective projection considering the aspect ratio.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The points to project.</p> required <code>camera_distance</code> <code>float</code> <p>The distance of the camera from the origin.</p> required <code>fov_deg</code> <code>float</code> <p>The field of view in degrees.</p> required <code>aspect_ratio</code> <code>float</code> <p>The aspect ratio of the screen. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>np.ndarray: The screen space coordinates of the projected</p> Source code in <code>simian/camera.py</code> <pre><code>def perspective_project(points, camera_distance, fov_deg, aspect_ratio=1.0):\n    \"\"\"\n    Project points onto a 2D plane using a perspective projection considering the aspect ratio.\n\n    Args:\n        points (np.ndarray): The points to project.\n        camera_distance (float): The distance of the camera from the origin.\n        fov_deg (float): The field of view in degrees.\n        aspect_ratio (float): The aspect ratio of the screen. Defaults to 1.0.\n\n    Returns:\n        np.ndarray: The screen space coordinates of the projected\n    \"\"\"\n    screen_points = []\n    fov_rad = math.radians(fov_deg)\n    f = 1.0 / math.tan(fov_rad / 2)\n    for point in points:\n        # Translate point to camera's frame of reference (camera along the positive x-axis)\n        p_cam = np.array([camera_distance - point[0], point[1], point[2]])\n        # Apply perspective projection if the point is in front of the camera\n        if p_cam[0] &gt; 0:\n            x = (p_cam[1] * f) / (p_cam[0] * aspect_ratio)  # Adjust x by aspect ratio\n            y = p_cam[2] * f / p_cam[0]\n            # Normalize to range [0, 1] for OpenGL screen space\n            screen_x = (x + 1) / 2\n            screen_y = (y + 1) / 2\n            screen_points.append((screen_x, screen_y))\n    return np.array(screen_points)\n</code></pre>"},{"location":"cameras/#simian.camera.position_camera","title":"<code>position_camera(combination, focus_object)</code>","text":"<p>Positions the camera based on the coverage factor and lens values.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>dict</code> <p>The combination dictionary containing coverage factor and lens values.</p> required <code>focus_object</code> <code>Object</code> <p>The object to focus the camera on.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/camera.py</code> <pre><code>def position_camera(combination: dict, focus_object: bpy.types.Object) -&gt; None:\n    \"\"\"\n    Positions the camera based on the coverage factor and lens values.\n\n    Args:\n        combination (dict): The combination dictionary containing coverage factor and lens values.\n        focus_object (bpy.types.Object): The object to focus the camera on.\n\n    Returns:\n        None\n    \"\"\"\n    camera = bpy.context.scene.objects[\"Camera\"]\n\n    # Get the bounding box of the focus object in world space\n    bpy.context.view_layer.update()\n\n    bbox = [\n        focus_object.matrix_world @ Vector(corner) for corner in focus_object.bound_box\n    ]\n    bbox_points = np.array([corner.to_tuple() for corner in bbox])\n\n    # Rotate points as per the desired view angle if any\n    # Assuming we want to compute this based on some predefined rotation angles\n    rotation_angles = (45, 45, 45)  # Example rotation angles\n    rotated_points = rotate_points(bbox_points, rotation_angles)\n\n    # scale rotated_points by combination[\"framing\"][\"coverage_factor\"]\n    rotated_points *= combination[\"framing\"][\"coverage_factor\"]\n\n    # Calculate the camera distance to frame the rotated bounding box correctly\n    fov_deg = combination[\"framing\"][\n        \"fov\"\n    ]  # Get the FOV from combination or default to 45\n    aspect_ratio = (\n        bpy.context.scene.render.resolution_x / bpy.context.scene.render.resolution_y\n    )\n\n    camera_distance, centroid, radius = compute_camera_distance(\n        rotated_points, fov_deg / aspect_ratio\n    )\n\n    # Set the camera properties\n    camera.data.angle = math.radians(fov_deg)  # Set camera FOV\n    if aspect_ratio &gt;= 1:\n        camera.data.sensor_fit = \"HORIZONTAL\"\n    else:\n        camera.data.sensor_fit = \"VERTICAL\"\n\n    bbox = [\n        focus_object.matrix_world @ Vector(corner) for corner in focus_object.bound_box\n    ]\n    bbox_min = min(bbox, key=lambda v: v.z)\n    bbox_max = max(bbox, key=lambda v: v.z)\n\n    # Calculate the height of the bounding box\n    bbox_height = bbox_max.z - bbox_min.z\n\n    # Position the camera based on the computed distance\n    camera.location = Vector((camera_distance, 0, 0))  # Adjust this as needed\n\n\n    # Set the position of the CameraAnimationRoot object to slightly above the focus object center, quasi-rule of thirds\n    # bbox_height / 2 is the center of the bounding box, bbox_height / 1.66 is more aesthetically pleasing\n    bpy.data.objects[\"CameraAnimationRoot\"].location = focus_object.location + Vector(\n        (0, 0, bbox_height/2)\n    )\n</code></pre>"},{"location":"cameras/#simian.camera.rotate_points","title":"<code>rotate_points(points, angles)</code>","text":"<p>Rotate points by given angles in degrees for (x, y, z) rotations.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>The points to rotate.</p> required <code>angles</code> <code>tuple</code> <p>The angles to rotate by in degrees for (x, y, z) rotations.</p> required <p>Returns:</p> Type Description <p>np.ndarray: The rotated points.</p> Source code in <code>simian/camera.py</code> <pre><code>def rotate_points(points, angles):\n    \"\"\"\n    Rotate points by given angles in degrees for (x, y, z) rotations.\n\n    Args:\n        points (np.ndarray): The points to rotate.\n        angles (tuple): The angles to rotate by in degrees for (x, y, z) rotations.\n\n    Returns:\n        np.ndarray: The rotated points.\n    \"\"\"\n\n    rotation = R.from_euler(\"xyz\", angles, degrees=True)\n    return np.array([rotation.apply(point) for point in points])\n</code></pre>"},{"location":"cameras/#simian.camera.set_camera_animation","title":"<code>set_camera_animation(combination, frame_interval, animation_length)</code>","text":"<p>Applies the specified animation to the camera based on the keyframes from the camera_data.json file. The total animation frames are fixed to ensure consistent speed.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>dict</code> <p>The combination dictionary containing animation data.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/camera.py</code> <pre><code>def set_camera_animation(combination: dict, frame_interval: int, animation_length: int) -&gt; None:\n    \"\"\"\n    Applies the specified animation to the camera based on the keyframes from the camera_data.json file.\n    The total animation frames are fixed to ensure consistent speed.\n\n    Args:\n        combination (dict): The combination dictionary containing animation data.\n\n    Returns:\n        None\n    \"\"\"\n    animation = combination[\"animation\"]\n    speed_factor = animation.get(\"speed_factor\", 1)/1.5\n    keyframes = animation[\"keyframes\"]\n    adjusted_frame_interval = frame_interval * (animation_length / 100)\n\n    for i, keyframe in enumerate(keyframes):\n        for obj_name, transforms in keyframe.items():\n            obj = bpy.data.objects.get(obj_name)\n            if obj is None:\n                raise ValueError(f\"Object {obj_name} not found in the scene\")\n            frame = int(i * adjusted_frame_interval)\n            for transform_name, value in transforms.items():\n                if transform_name == \"position\":\n                    obj.location = [coord * speed_factor for coord in value]\n                    obj.keyframe_insert(data_path=\"location\", frame=frame)\n                elif transform_name == \"rotation\":\n                    obj.rotation_euler = [\n                        math.radians(angle * speed_factor) for angle in value\n                    ]\n                    obj.keyframe_insert(data_path=\"rotation_euler\", frame=frame)\n                elif transform_name == \"scale\":\n                    obj.scale = [coord * speed_factor for coord in value]\n                    obj.keyframe_insert(data_path=\"scale\", frame=frame)\n                elif transform_name == \"angle_offset\" and obj_name == \"Camera\":\n                    camera_data = bpy.data.objects[\"Camera\"].data\n                    camera_data.angle = math.radians(\n                        combination[\"framing\"][\"fov\"] + value\n                    )\n                    camera_data.keyframe_insert(data_path=\"lens\", frame=frame)\n\n    bpy.context.scene.frame_set(0)\n</code></pre>"},{"location":"cameras/#simian.camera.set_camera_settings","title":"<code>set_camera_settings(combination)</code>","text":"<p>Applies camera settings from a combination to the Blender scene.</p> <p>This function updates various camera settings including orientation, pivot adjustments, and framing based on the provided combination dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>dict</code> <p>A dictionary containing camera settings including 'fov', 'animation',                 and orientation details.</p> required <p>Returns:     None</p> Source code in <code>simian/camera.py</code> <pre><code>def set_camera_settings(combination: dict) -&gt; None:\n    \"\"\"\n    Applies camera settings from a combination to the Blender scene.\n\n    This function updates various camera settings including orientation, pivot adjustments, and\n    framing based on the provided combination dictionary.\n\n    Args:\n        combination (dict): A dictionary containing camera settings including 'fov', 'animation',\n                            and orientation details.\n    Returns:\n        None\n    \"\"\"\n    camera = bpy.context.scene.objects[\"Camera\"]\n    camera_data = camera.data\n\n    postprocessing = combination.get(\"postprocessing\", {})\n\n    # Apply bloom settings\n    bloom_settings = postprocessing.get(\"bloom\", {})\n    threshold = bloom_settings.get(\"threshold\", 0.8)\n    intensity = bloom_settings.get(\"intensity\", 0.5)\n    radius = bloom_settings.get(\"radius\", 5.0)\n    bpy.context.scene.eevee.use_bloom = True\n    bpy.context.scene.eevee.bloom_threshold = threshold\n    bpy.context.scene.eevee.bloom_intensity = intensity\n    bpy.context.scene.eevee.bloom_radius = radius\n\n    # Apply SSAO settings\n    ssao_settings = postprocessing.get(\"ssao\", {})\n    distance = ssao_settings.get(\"distance\", 0.2)\n    factor = ssao_settings.get(\"factor\", 0.5)\n    bpy.context.scene.eevee.use_gtao = True\n    bpy.context.scene.eevee.gtao_distance = distance\n    bpy.context.scene.eevee.gtao_factor = factor\n\n    # Apply SSRR settings\n    ssrr_settings = postprocessing.get(\"ssrr\", {})\n    max_roughness = ssrr_settings.get(\"max_roughness\", 0.5)\n    thickness = ssrr_settings.get(\"thickness\", 0.1)\n    bpy.context.scene.eevee.use_ssr = True\n    bpy.context.scene.eevee.use_ssr_refraction = True\n    bpy.context.scene.eevee.ssr_max_roughness = max_roughness\n    bpy.context.scene.eevee.ssr_thickness = thickness\n\n    # Apply motion blur settings\n    motionblur_settings = postprocessing.get(\"motionblur\", {})\n    shutter_speed = motionblur_settings.get(\"shutter_speed\", 0.5)\n    bpy.context.scene.eevee.use_motion_blur = True\n    bpy.context.scene.eevee.motion_blur_shutter = shutter_speed\n\n    # Get the initial lens value from the combination\n    initial_lens = combination[\"framing\"][\"fov\"]\n\n    # Get the first keyframe's angle_offset value, if available\n    animation = combination[\"animation\"]\n    keyframes = animation[\"keyframes\"]\n    if (\n        keyframes\n        and \"Camera\" in keyframes[0]\n        and \"angle_offset\" in keyframes[0][\"Camera\"]\n    ):\n        angle_offset = keyframes[0][\"Camera\"][\"angle_offset\"]\n        camera_data.angle = math.radians(initial_lens + angle_offset)\n    else:\n        camera_data.angle = math.radians(initial_lens)\n\n    orientation_data = combination[\"orientation\"]\n    orientation = {\"pitch\": orientation_data[\"pitch\"], \"yaw\": orientation_data[\"yaw\"]}\n\n    # Rotate CameraOrientationPivotYaw by the Y\n    camera_orientation_pivot_yaw = bpy.data.objects.get(\"CameraOrientationPivotYaw\")\n    camera_orientation_pivot_yaw.rotation_euler[2] = orientation[\"yaw\"] * math.pi / 180\n\n    # Rotate CameraOrientationPivotPitch by the X\n    camera_orientation_pivot_pitch = bpy.data.objects.get(\"CameraOrientationPivotPitch\")\n    camera_orientation_pivot_pitch.rotation_euler[1] = (\n        orientation[\"pitch\"] * -math.pi / 180\n    )\n\n    # set the camera framerate to 30\n    bpy.context.scene.render.fps = 30\n</code></pre>"},{"location":"combiner/","title":"Combiner","text":"<p>The <code>combiner</code> module is responible for creating the combinations.json file which is used to store the combinations of assets that will be used to render the final video. It handles reading the assets from the assets directory, creating the combinations, and writing the combinations to the combinations.json file.</p>"},{"location":"combiner/#simian.combiner.add_camera_follow","title":"<code>add_camera_follow(objects, camera_follow)</code>","text":"<p>Add camera follow to objects.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <p>List of objects.</p> required <code>camera_follow</code> <p>Camera follow flag.</p> required <p>Returns:</p> Type Description <p>List[Dict[str, Any]]: List of objects with camera follow.</p> Source code in <code>simian/combiner.py</code> <pre><code>def add_camera_follow(objects, camera_follow):\n    \"\"\"\n    Add camera follow to objects.\n\n    Args:\n        objects: List of objects.\n        camera_follow: Camera follow flag.\n\n    Returns:\n        List[Dict[str, Any]]: List of objects with camera follow.\n    \"\"\"\n    if camera_follow:  \n        # how many objects in array\n        num_objects = len(objects)\n        # randomly select an object to follow\n        random_index = random.randint(0, num_objects - 1)\n        objects[random_index][\"camera_follow\"] = {\"follow\": True}\n\n    return objects\n</code></pre>"},{"location":"combiner/#simian.combiner.flatten_descriptions","title":"<code>flatten_descriptions(descriptions)</code>","text":"<p>Flatten a list of descriptions, which may contain nested lists. Copy codeArgs:     descriptions (List[Any]): A list of descriptions which may contain nested lists.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A flattened list of descriptions.</p> Source code in <code>simian/combiner.py</code> <pre><code>def flatten_descriptions(descriptions: List[Any]) -&gt; List[str]:\n    \"\"\"\n    Flatten a list of descriptions, which may contain nested lists.\n    Copy codeArgs:\n        descriptions (List[Any]): A list of descriptions which may contain nested lists.\n\n    Returns:\n        List[str]: A flattened list of descriptions.\n    \"\"\"\n    flat_list = []\n    for item in descriptions:\n        if isinstance(item, list):\n            flat_list.extend(flatten_descriptions(item))\n        else:\n            flat_list.append(item)\n    return flat_list\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_animation","title":"<code>generate_animation(camera_data)</code>","text":"<p>Generate camera animation based on the camera data. Copy codeArgs:     camera_data (Dict[str, Any]): Camera data.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Camera animation.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_animation(camera_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate camera animation based on the camera data.\n    Copy codeArgs:\n        camera_data (Dict[str, Any]): Camera data.\n\n    Returns:\n        Dict[str, Any]: Camera animation.\n    \"\"\"\n    animation = random.choice(camera_data[\"animations\"])\n    animation = animation.copy()\n    animation[\"speed_factor\"] = random.uniform(0.5, 2.0)\n    animation.pop(\"descriptions\", None)\n    return animation\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_background","title":"<code>generate_background(background_dict, background_names, background_weights)</code>","text":"<p>Generate a random background.</p> <p>Parameters:</p> Name Type Description Default <code>background_dict</code> <p>Background data.</p> required <code>background_names</code> <p>List of background names.</p> required <code>background_weigh</code> <code>ts</code> <p>List of background</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Generated background.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_background(\n    background_dict, background_names, background_weights\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a random background.\n\n    Args:\n        background_dict: Background data.\n        background_names: List of background names.\n        background_weigh ts: List of background\n\n    Returns:\n        Dict[str, Any]: Generated background.\n    \"\"\"\n    chosen_background = random.choices(background_names, weights=background_weights)[0]\n    # Get the keys from the chosen background\n    background_keys = list(background_dict[chosen_background].keys())\n    background_id = random.choice(background_keys)\n    bg = background_dict[chosen_background][background_id]\n\n    background = {\n        \"name\": bg[\"name\"],\n        \"url\": bg[\"url\"],\n        \"id\": background_id,\n        \"from\": chosen_background,\n    }\n\n    return background\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_camerafollow_captions","title":"<code>generate_camerafollow_captions(combination, camera_data)</code>","text":"<p>Generate captions for camera following objects based on the combination data.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>Dict[str, Any]</code> <p>Combination data.</p> required <code>camera_data</code> <code>Dict[str, Any]</code> <p>Camera data containing camera follow options. </p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of camera follow captions.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_camerafollow_captions(combination: Dict[str, Any], camera_data) -&gt; List[str]:\n    \"\"\"\n    Generate captions for camera following objects based on the combination data.\n\n    Args:\n        combination (Dict[str, Any]): Combination data.\n        camera_data (Dict[str, Any]): Camera data containing camera follow options. \n\n    Returns:\n        List[str]: List of camera follow captions.\n    \"\"\"\n    if 'camera_follow' not in camera_data:\n        return []\n\n    camera_follow_options = camera_data['camera_follow']\n    camera_follow_captions = []\n    for obj in combination['objects']:\n        if 'camera_follow' in obj:\n            caption = random.choice(camera_follow_options)\n            caption = caption.replace('&lt;object&gt;', obj['name'])\n            camera_follow_captions.append(caption)        \n    return camera_follow_captions\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_fov_caption","title":"<code>generate_fov_caption(combination)</code>","text":"<p>Generate a caption for the field of view (FOV) based on the combination data.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>Dict[str, Any]</code> <p>Combination data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>FOV caption.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_fov_caption(combination: Dict[str, Any]) -&gt; str:\n    \"\"\"\n    Generate a caption for the field of view (FOV) based on the combination data.\n\n    Args:\n        combination (Dict[str, Any]): Combination data.\n\n    Returns:\n        str: FOV caption.\n    \"\"\"\n\n    fov_templates = {\n        \"degrees\": [\n            \"The camera has a &lt;fov&gt; degree field of view.\",\n            \"The camera has a &lt;fov&gt; degree FOV.\",\n            \"The field of view is &lt;fov&gt; degrees.\",\n            \"Set the fov of the camera to &lt;fov&gt; degrees.\",\n            \"Set the FOV of the camera to &lt;fov&gt;\u00b0\",\n        ],\n        \"mm\": [\n            \"The camera has a &lt;mm&gt; mm focal length.\",\n            \"The focal length is &lt;mm&gt; mm.\",\n            \"Set the focal length of the camera to &lt;mm&gt; mm.\",\n        ],\n    }\n\n    fov = combination[\"framing\"][\"fov\"]\n\n    # FOV is stored as degrees in the framing data\n    fov_types = \"degrees\", \"mm\"\n\n    # Select a random FOV type\n    fov_type = random.choice(fov_types)\n\n    # Select a random FOV template\n    fov_template = random.choice(fov_templates[fov_type])\n\n    # Replace the &lt;fov&gt; placeholder with the FOV value\n    fov_template = fov_template.replace(\"&lt;fov&gt;\", str(fov))\n\n    # Convert FOV to focal length\n    focal_length = int(35 / (2 * math.tan(math.radians(fov) / 2)))\n    fov_caption = fov_template.replace(\"&lt;mm&gt;\", str(focal_length))\n\n    if fov_type == \"degrees\":\n        fov_caption += f\" ({focal_length:.2f} mm focal length)\"\n\n    return fov_caption\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_framing","title":"<code>generate_framing(camera_data)</code>","text":"<p>Generate camera framing based on the camera data. Copy codeArgs:     camera_data (Dict[str, Any]): Camera data.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Camera framing.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_framing(camera_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate camera framing based on the camera data.\n    Copy codeArgs:\n        camera_data (Dict[str, Any]): Camera data.\n\n    Returns:\n        Dict[str, Any]: Camera framing.\n    \"\"\"\n    # Get the min_fov and max_fov across all framings\n    fov_min = min([f[\"fov_min\"] for f in camera_data[\"framings\"]])\n    fov_max = max([f[\"fov_max\"] for f in camera_data[\"framings\"]])\n\n    # Randomly roll an FOV value between FOV_min and FOV_max\n    fov = int(random.uniform(fov_min, fov_max))\n\n    # Find the corresponding framing\n    framing = None\n    for f in camera_data[\"framings\"]:\n        if f[\"fov_min\"] &lt;= fov &lt;= f[\"fov_max\"]:\n            framing = f\n            break\n\n    # Derive a coverage_factor between coverage_factor_min and coverage_factor_max\n    coverage_factor = random.uniform(\n        framing[\"coverage_factor_min\"], framing[\"coverage_factor_max\"]\n    )\n\n    framing = {\n        \"fov\": fov,\n        \"coverage_factor\": coverage_factor,\n        \"name\": framing[\"name\"],\n    }\n    return framing\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_framing_caption","title":"<code>generate_framing_caption(camera_data, combination)</code>","text":"<p>Generate a caption for framing based on the camera data and combination data. Copy codeArgs:     camera_data (Dict[str, Any]): Camera data.     combination (Dict[str, Any]): Combination data.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Framing caption.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_framing_caption(\n    camera_data: Dict[str, Any], combination: Dict[str, Any]\n) -&gt; str:\n    \"\"\"\n    Generate a caption for framing based on the camera data and combination data.\n    Copy codeArgs:\n        camera_data (Dict[str, Any]): Camera data.\n        combination (Dict[str, Any]): Combination data.\n\n    Returns:\n        str: Framing caption.\n    \"\"\"\n    framing = combination[\"framing\"]\n    framing_name = framing[\"name\"]\n\n    # Find the matching framing in camera_data\n    matching_framing = next(\n        (f for f in camera_data[\"framings\"] if f[\"name\"] == framing_name), None\n    )\n\n    if matching_framing:\n        framing_description = random.choice(matching_framing[\"descriptions\"])\n        framing_description = framing_description.replace(\n            \"&lt;fov&gt;\", str(framing[\"fov\"])\n        ).replace(\"&lt;coverage_factor&gt;\", str(framing[\"coverage_factor\"]))\n        return framing_description\n    else:\n        return \"\"\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_movement_captions","title":"<code>generate_movement_captions(combination, object_data)</code>","text":"<p>Generate captions for object movement based on the combination data.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>Dict[str, Any]</code> <p>Combination data.</p> required <code>object_data</code> <code>Dict[str, Any]</code> <p>Object data including movement templates and speed descriptions.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of movement captions.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_movement_captions(combination: Dict[str, Any], object_data) -&gt; List[str]:\n    \"\"\"\n    Generate captions for object movement based on the combination data.\n\n    Args:\n        combination (Dict[str, Any]): Combination data.\n        object_data (Dict[str, Any]): Object data including movement templates and speed descriptions.\n\n    Returns:\n        List[str]: List of movement captions.\n    \"\"\"\n\n    if 'movement_description_relationship' not in object_data:\n        return []\n\n    object_movement_data = object_data['movement_description_relationship']\n    object_movement_speed_words = object_data['movement_speed_description']\n\n    movement_captions = []\n    for obj in combination['objects']:\n        if 'movement' in obj:\n            speed = obj['movement']['speed']\n            if speed &lt;= 0.25:\n                speed_words = object_movement_speed_words['0.25']\n            else:\n                speed_words = object_movement_speed_words['0.5']\n\n            speed_description = random.choice(speed_words)\n\n            template = random.choice(object_movement_data)\n            movement_description = template.replace('&lt;object&gt;', obj['name'])\n            movement_description = movement_description.replace('&lt;movement&gt;', obj['movement']['direction'])\n            movement_description = movement_description.replace('&lt;speed&gt;', f'{speed:.2f}')\n\n            if '&lt;speed_description&gt;' in movement_description:\n                movement_description = movement_description.replace('&lt;speed_description&gt;', speed_description)\n\n            movement_captions.append(movement_description)\n\n    return movement_captions\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_object_name_description_captions","title":"<code>generate_object_name_description_captions(combination, object_data)</code>","text":"<p>Generate captions for object names and descriptions based on the combination data.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>Dict[str, Any]</code> <p>Combination data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Object name and description captions.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_object_name_description_captions(\n    combination: Dict[str, Any], object_data\n) -&gt; str:\n    \"\"\"\n    Generate captions for object names and descriptions based on the combination data.\n\n    Args:\n        combination (Dict[str, Any]): Combination data.\n\n    Returns:\n        str: Object name and description captions.\n    \"\"\"\n    object_name_descriptions = []\n    for obj in combination[\"objects\"]:\n        object_description = obj[\"description\"]\n\n        object_scale = obj[\"scale\"]\n        scale_factor = object_scale[\"factor\"]\n        scale_name = object_scale[\"name_synonym\"]\n\n        object_name_description_relationship = random.choice(\n            object_data[\"name_description_relationship\"]\n        )\n\n        # Replace placeholders with actual values\n        object_name_description_relationship = (\n            object_name_description_relationship.replace(\"&lt;name&gt;\", object_description)\n            .replace(\"&lt;size&gt;\", scale_name, 1)\n        )\n\n        random_metric_m = random.choice([\"meters\", \"m\", \"\"])\n        size_in_meters = f\"{scale_factor}{random_metric_m}\"\n        object_name_description_relationship = (\n            object_name_description_relationship.replace(\n                \"&lt;size_in_meters&gt;\", size_in_meters, 1\n            )\n        )\n\n        random_metric_f = random.choice([\"feet\", \"ft\", \"\"])\n        size_in_feet = f\"{meters_to_feet_rounded(scale_factor)}{random_metric_f}\"\n        object_name_description_relationship = (\n            object_name_description_relationship.replace(\n                \"&lt;size_in_feet&gt;\", size_in_feet, 1\n            )\n        )\n\n        object_name_descriptions.append(object_name_description_relationship)\n\n    random.shuffle(object_name_descriptions)\n    object_name_descriptions = \" \".join(object_name_descriptions)\n\n    return object_name_descriptions\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_objects","title":"<code>generate_objects(object_data, dataset_names, dataset_weights, dataset_dict, captions_data, ontop_data)</code>","text":"<p>Generate a list of random objects.</p> <p>Parameters:</p> Name Type Description Default <code>object_data</code> <code>Dict[str, Any]</code> <p>Object data.</p> required <code>dataset_names</code> <code>List[str]</code> <p>List of dataset names.</p> required <code>dataset_weights</code> <code>List[int]</code> <p>List of dataset weights.</p> required <code>dataset_dict</code> <code>Dict[str, Any]</code> <p>Dataset dictionary.</p> required <code>captions_data</code> <code>Dict[str, Any]</code> <p>Captions data.</p> required <code>ontop_data</code> <code>str</code> <p>Flag indicating whether to allow objects on top of each other.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: List of generated objects.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_objects(\n    object_data, dataset_names, dataset_weights, dataset_dict, captions_data, ontop_data\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Generate a list of random objects.\n\n    Args:\n        object_data (Dict[str, Any]): Object data.\n        dataset_names (List[str]): List of dataset names.\n        dataset_weights (List[int]): List of dataset weights.\n        dataset_dict (Dict[str, Any]): Dataset dictionary.\n        captions_data (Dict[str, Any]): Captions data.\n        ontop_data (str): Flag indicating whether to allow objects on top of each other.\n\n    Returns:\n        List[Dict[str, Any]]: List of generated objects.\n    \"\"\"\n    chosen_dataset = \"cap3d\"\n\n    if chosen_dataset not in dataset_dict:\n        raise KeyError(f\"Dataset '{chosen_dataset}' not found in dataset_dict\")\n\n    # Randomly generate max_number_of_objects\n    max_number_of_objects = parse_args().max_number_of_objects\n    number_of_objects = random.randint(1, max_number_of_objects)\n\n    object_scales = object_data[\"scales\"]\n\n    # Scale values\n    scale_values = [scale[\"factor\"] for scale in object_scales.values()]\n\n    # Create simple triangular distribution based on scale_values\n    len_scale_values = len(scale_values)\n    mid_point = len_scale_values // 2\n    if len_scale_values % 2 == 0:\n        weights = [i + 1 for i in range(mid_point)] + [\n            mid_point - i for i in range(mid_point)\n        ]\n    else:\n        weights = (\n            [i + 1 for i in range(mid_point)]\n            + [mid_point + 1]\n            + [mid_point - i for i in range(mid_point)]\n        )\n\n    total_weight = sum(weights)\n    normalized_weights = [w / total_weight for w in weights]\n\n    objects = []\n    positions_taken = set()\n    for i in range(number_of_objects):\n        object_uid = random.choice(dataset_dict[chosen_dataset])\n        object_description = captions_data[object_uid]\n        object_description = captions_data[object_uid].rstrip('.')  # Remove trailing period\n\n        scale_choice = random.choices(\n            list(object_scales.items()), weights=normalized_weights, k=1\n        )[0]\n        scale_key = scale_choice[0]\n        scale_value = scale_choice[1]\n\n        scale = {\n            \"factor\": scale_value[\"factor\"],\n            \"name\": scale_key,\n            \"name_synonym\": random.choice(scale_value[\"names\"]),\n        }\n\n        if i == 0:\n            placement = 4  # Ensure the first object is always placed at position 4\n            positions_taken.add(placement)\n        else:\n            possible_positions = [\n                pos for pos in range(0, 9) if pos not in positions_taken or ontop_data\n            ]\n            placement = random.choice(possible_positions)\n            positions_taken.add(placement)\n\n        object = {\n            \"name\": object_description,  # Use the description directly\n            \"uid\": object_uid,\n            \"description\": object_description,  # Use the description directly\n            \"placement\": placement,\n            \"from\": chosen_dataset,\n            \"scale\": scale,\n        }\n\n        objects.append(object)\n\n    return objects\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_ontop_captions","title":"<code>generate_ontop_captions(combination, ontop_data, object_data)</code>","text":"<p>Generate captions for objects being on top of each other based on the combination data.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>Dict[str, Any]</code> <p>Combination data.</p> required <code>ontop_data</code> <code>str</code> <p>Flag indicating whether to allow objects on top of each other.</p> required <code>object_data</code> <code>Dict[str, Any]</code> <p>Object data containing ontop description relationships.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of ontop captions.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_ontop_captions(combination: Dict[str, Any], ontop_data, object_data) -&gt; List[str]:\n    \"\"\"\n    Generate captions for objects being on top of each other based on the combination data.\n\n    Args:\n        combination (Dict[str, Any]): Combination data.\n        ontop_data (str): Flag indicating whether to allow objects on top of each other.\n        object_data (Dict[str, Any]): Object data containing ontop description relationships.\n\n    Returns:\n        List[str]: List of ontop captions.\n    \"\"\"\n    if not ontop_data or 'ontop_description_relationship' not in object_data:\n        return []\n\n    ontop_captions = []\n    placement_stacks = {}\n\n    object_ontop_captions = object_data['ontop_description_relationship']\n\n    # Maintain the original order of objects\n    for obj in combination[\"objects\"]:\n        placement = obj[\"placement\"]\n        if placement not in placement_stacks:\n            placement_stacks[placement] = []\n        placement_stacks[placement].append(obj)\n\n    # Generate captions for objects at the same placement\n    for placement, objects in placement_stacks.items():\n        if len(objects) &gt; 1:\n            for i in range(len(objects) - 1):\n                below_obj = objects[i]\n                above_obj = objects[i + 1]\n\n                caption_template = random.choice(object_ontop_captions)\n\n                # Always describe from bottom to top to maintain consistency\n                caption = caption_template.replace(\"&lt;object1&gt;\", above_obj['name']).replace(\"&lt;object2&gt;\", below_obj['name'])\n\n                ontop_captions.append(caption)\n\n    return ontop_captions\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_orientation","title":"<code>generate_orientation(camera_data, objects, background)</code>","text":"<p>Generate camera orientation based on the camera data, objects, and background. Copy codeArgs:     camera_data (Dict[str, Any]): Camera data.     objects (List[Dict[str, Any]]): List of objects in the scene.     background (Dict[str, Any]): Background information.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dict[str, int]: Camera orientation.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_orientation(\n    camera_data: Dict[str, Any],\n    objects: List[Dict[str, Any]],\n    background: Dict[str, Any],\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Generate camera orientation based on the camera data, objects, and background.\n    Copy codeArgs:\n        camera_data (Dict[str, Any]): Camera data.\n        objects (List[Dict[str, Any]]): List of objects in the scene.\n        background (Dict[str, Any]): Background information.\n\n    Returns:\n        Dict[str, int]: Camera orientation.\n    \"\"\"\n    orientation_data = camera_data[\"orientation\"]\n\n    # Roll a number between orientation['yaw_min'] and orientation['yaw_max']\n    yaw = random.randint(orientation_data[\"yaw_min\"], orientation_data[\"yaw_max\"])\n    pitch = random.randint(orientation_data[\"pitch_min\"], orientation_data[\"pitch_max\"])\n\n    # Check if the camera is going to be occluded by the objects\n    # If so, re-roll the orientation until a non-occluded orientation is found\n    while True:\n        occluded = False\n        for obj in objects[1:]:\n            # Calculate the vector from the camera to the object\n            # calculate the position of the camera on the unit circle from yaw\n            camera_x = math.cos(math.radians(yaw))\n            camera_y = math.sin(math.radians(yaw))\n\n            camera_vector = [camera_x, camera_y, 0]\n\n            # get the position of the object\n            object_position = obj[\"transformed_position\"]\n\n            # normalize the object position, account for division by 0\n            object_position = [\n                a / max(1e-6, sum([b**2 for b in object_position])) ** 0.5\n                for a in object_position\n            ]\n\n            # if the magnitude of the object position is 0, ignore it\n            if sum([a**2 for a in object_position]) &lt; 0.001:\n                continue\n\n            # dot product of the camera vector and the object position\n            dot_product = sum([a * b for a, b in zip(camera_vector, object_position)])\n\n            # calculate an angle padding on both sides of the object to use as a threshold\n            angle = math.radians(15)\n            # set the threshold to the dot product - cos of the angle\n            threshold = math.cos(angle)\n\n            if dot_product &gt; threshold:\n                occluded = True\n                break\n\n        if not occluded:\n            break\n\n        # Re-roll the orientation if occluded and try again\n        yaw = random.randint(orientation_data[\"yaw_min\"], orientation_data[\"yaw_max\"])\n        pitch = random.randint(\n            orientation_data[\"pitch_min\"], orientation_data[\"pitch_max\"]\n        )\n\n    orientation = {\n        \"yaw\": int(yaw),\n        \"pitch\": int(pitch),\n    }\n\n    return orientation\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_orientation_caption","title":"<code>generate_orientation_caption(camera_data, combination)</code>","text":"<p>Generate a caption for the camera orientation based on the combination data.</p> <p>Parameters:</p> Name Type Description Default <code>camera_data</code> <code>Dict[str, Any]</code> <p>Camera data.</p> required <code>combination</code> <code>Dict[str, Any]</code> <p>Combination data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Orientation caption.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_orientation_caption(\n    camera_data: Dict[str, Any], combination: Dict[str, Any]\n) -&gt; str:\n    \"\"\"\n    Generate a caption for the camera orientation based on the combination data.\n\n    Args:\n        camera_data (Dict[str, Any]): Camera data.\n        combination (Dict[str, Any]): Combination data.\n\n    Returns:\n        str: Orientation caption.\n    \"\"\"\n    pitch_labels = camera_data[\"orientation\"][\"labels\"][\"pitch\"]\n    yaw_labels = camera_data[\"orientation\"][\"labels\"][\"yaw\"]\n\n    closest_pitch_label = min(\n        pitch_labels.keys(),\n        key=lambda x: abs(int(x) - int(combination[\"orientation\"][\"pitch\"])),\n    )\n    closest_yaw_label = min(\n        yaw_labels.keys(),\n        key=lambda x: abs(int(x) - int(combination[\"orientation\"][\"yaw\"])),\n    )\n\n    # Replace the placeholders in the camera text with the closest matching labels\n    orientation_text = random.choice(camera_data[\"orientation\"][\"descriptions\"])\n    orientation_text = (\n        orientation_text.replace(\n            \"&lt;pitch&gt;\", random.choice(pitch_labels[closest_pitch_label])\n        )\n        .replace(\"&lt;degrees&gt;\", str(combination[\"orientation\"][\"pitch\"]))\n        .replace(\"&lt;yaw&gt;\", random.choice(yaw_labels[closest_yaw_label]))\n        .replace(\"&lt;degrees&gt;\", str(combination[\"orientation\"][\"yaw\"]))\n    )\n\n    return orientation_text\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_postprocessing","title":"<code>generate_postprocessing(camera_data)</code>","text":"<p>Generate postprocessing settings based on the camera data. Copy codeArgs:     camera_data (Dict[str, Any]): Camera data.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Postprocessing settings.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_postprocessing(camera_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate postprocessing settings based on the camera data.\n    Copy codeArgs:\n        camera_data (Dict[str, Any]): Camera data.\n\n    Returns:\n        Dict[str, Any]: Postprocessing settings.\n    \"\"\"\n    postprocessing = {}\n\n    bloom_data = camera_data[\"postprocessing\"][\"bloom\"]\n\n    bloom_threshold = random.uniform(\n        bloom_data[\"threshold_min\"], bloom_data[\"threshold_max\"]\n    )\n    bloom_intensity = random.uniform(\n        bloom_data[\"intensity_min\"], bloom_data[\"intensity_max\"]\n    )\n    bloom_radius = random.uniform(bloom_data[\"radius_min\"], bloom_data[\"radius_max\"])\n\n    bloom_type = \"none\"\n    bloom_types = bloom_data[\"types\"]\n    for t in bloom_types.keys():\n        if (\n            bloom_intensity &gt;= bloom_types[t][\"intensity_min\"]\n            and bloom_intensity &lt;= bloom_types[t][\"intensity_max\"]\n        ):\n            bloom_type = t\n            break\n\n    postprocessing[\"bloom\"] = {\n        \"threshold\": bloom_threshold,\n        \"intensity\": bloom_intensity,\n        \"radius\": bloom_radius,\n        \"type\": bloom_type,\n    }\n\n    ssao_data = camera_data[\"postprocessing\"][\"ssao\"]\n    ssao_distance = random.uniform(ssao_data[\"distance_min\"], ssao_data[\"distance_max\"])\n    ssao_factor = random.uniform(ssao_data[\"factor_min\"], ssao_data[\"factor_max\"])\n\n    ssao_type = \"none\"\n    for t in ssao_data[\"types\"].keys():\n        if (\n            ssao_factor &gt;= ssao_data[\"types\"][t][\"factor_min\"]\n            and ssao_factor &lt;= ssao_data[\"types\"][t][\"factor_max\"]\n        ):\n            ssao_type = t\n            break\n\n    postprocessing[\"ssao\"] = {\n        \"distance\": ssao_distance,\n        \"factor\": ssao_factor,\n        \"type\": ssao_type,\n    }\n\n    ssrr_data = camera_data[\"postprocessing\"][\"ssrr\"]\n    ssrr_max_roughness = random.uniform(\n        ssrr_data[\"min_max_roughness\"], ssrr_data[\"max_max_roughness\"]\n    )\n    ssrr_thickness = random.uniform(\n        ssrr_data[\"min_thickness\"], ssrr_data[\"max_thickness\"]\n    )\n\n    ssrr_type = \"none\"\n    for t in ssrr_data[\"types\"].keys():\n        if (\n            ssrr_max_roughness &gt;= ssrr_data[\"types\"][t][\"max_roughness_min\"]\n            and ssrr_max_roughness &lt;= ssrr_data[\"types\"][t][\"max_roughness_max\"]\n        ):\n            ssrr_type = t\n            break\n\n    postprocessing[\"ssrr\"] = {\n        \"max_roughness\": ssrr_max_roughness,\n        \"thickness\": ssrr_thickness,\n        \"type\": ssrr_type,\n    }\n\n    motionblur_data = camera_data[\"postprocessing\"][\"motionblur\"]\n    motionblur_shutter_speed = random.uniform(\n        motionblur_data[\"shutter_speed_min\"], motionblur_data[\"shutter_speed_max\"]\n    )\n\n    motionblur_type = \"none\"\n    for t in motionblur_data[\"types\"].keys():\n        if (\n            motionblur_shutter_speed &gt;= motionblur_data[\"types\"][t][\"shutter_speed_min\"]\n            and motionblur_shutter_speed\n            &lt;= motionblur_data[\"types\"][t][\"shutter_speed_max\"]\n        ):\n            motionblur_type = t\n            break\n\n    postprocessing[\"motionblur\"] = {\n        \"shutter_speed\": motionblur_shutter_speed,\n        \"type\": motionblur_type,\n    }\n    return postprocessing\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_postprocessing_caption","title":"<code>generate_postprocessing_caption(combination, camera_data)</code>","text":"<p>Generate a caption for postprocessing based on the combination data.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>Dict[str, Any]</code> <p>Combination data.</p> required <code>camera_data</code> <code>Dict[str, Any]</code> <p>Camera data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Postprocessing caption.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_postprocessing_caption(combination: Dict[str, Any], camera_data) -&gt; str:\n    \"\"\"\n    Generate a caption for postprocessing based on the combination data.\n\n    Args:\n        combination (Dict[str, Any]): Combination data.\n        camera_data (Dict[str, Any]): Camera data.\n\n    Returns:\n        str: Postprocessing caption.\n    \"\"\"\n    postprocessing = combination[\"postprocessing\"]\n    caption_parts = []\n\n    postprocessing_options = camera_data[\"postprocessing\"]\n\n    for key in postprocessing:\n        post_type = postprocessing[key][\"type\"]\n        if key == \"bloom\":\n            bloom_caption = random.choice(\n                postprocessing_options[\"bloom\"][\"types\"][post_type][\"descriptions\"]\n            )\n            caption_parts.append(bloom_caption)\n        elif key == \"ssao\":\n            ssao_caption = random.choice(\n                postprocessing_options[\"ssao\"][\"types\"][post_type][\"descriptions\"]\n            )\n            caption_parts.append(ssao_caption)\n        elif key == \"ssrr\":\n            ssrr_caption = random.choice(\n                postprocessing_options[\"ssrr\"][\"types\"][post_type][\"descriptions\"]\n            )\n            caption_parts.append(ssrr_caption)\n        elif key == \"motionblur\":\n            motionblur_caption = random.choice(\n                postprocessing_options[\"motionblur\"][\"types\"][post_type][\"descriptions\"]\n            )\n            caption_parts.append(motionblur_caption)\n\n    # Ensure we have at least one caption\n    if not caption_parts:\n        return \"No significant post-processing effects applied.\"\n\n    # Ensure at least 1-2 captions remain\n    min_captions = min(2, len(caption_parts))\n    if len(caption_parts) &gt; min_captions:\n        num_to_pop = random.randint(1, len(caption_parts) - min_captions)\n        for _ in range(num_to_pop):\n            random_index_to_remove = random.randint(0, len(caption_parts) - 1)\n            caption_parts.pop(random_index_to_remove)\n\n    return \" \".join(caption_parts)\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_relationship_captions","title":"<code>generate_relationship_captions(combination)</code>","text":"<p>Generate captions for object relationships based on the combination data.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>Dict[str, Any]</code> <p>Combination data.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of relationship captions.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_relationship_captions(combination: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"\n    Generate captions for object relationships based on the combination data.\n\n    Args:\n        combination (Dict[str, Any]): Combination data.\n\n    Returns:\n        List[str]: List of relationship captions.\n    \"\"\"\n    threshold_relationships = len(combination[\"objects\"])\n\n    adjusted_objects = adjust_positions(\n        combination[\"objects\"], combination[\"orientation\"][\"yaw\"]\n    )\n\n    camera_yaw = combination[\"orientation\"][\"yaw\"]\n\n    relationships = determine_relationships(adjusted_objects, camera_yaw)\n\n    # Write relationships into the JSON file with combination[\"objects\"][\"relationships\"]\n    for i, obj in enumerate(combination[\"objects\"]):\n        obj.setdefault(\"relationships\", [])\n        if i &lt; len(relationships):\n            obj[\"relationships\"] = relationships[i]\n\n    selected_relationships = relationships\n    if threshold_relationships &lt; len(relationships):\n        selected_relationships = random.sample(relationships, threshold_relationships)\n\n    return selected_relationships\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_stage","title":"<code>generate_stage(texture_data)</code>","text":"<p>Generate a random stage.</p> <p>Parameters:</p> Name Type Description Default <code>texture_data</code> <p>Texture data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Generated stage.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_stage(texture_data) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generate a random stage.\n\n    Args:\n        texture_data: Texture data.\n\n    Returns:\n        Dict[str, Any]: Generated stage.\n    \"\"\"\n    texture_names = list(texture_data.keys())\n    texture_weights = [len(texture_data[name][\"maps\"]) for name in texture_names]\n    chosen_texture = random.choices(texture_names, weights=texture_weights)[0]\n    maps = texture_data[chosen_texture][\"maps\"]\n\n    material = {\n        \"name\": texture_data[chosen_texture][\"name\"],\n        \"maps\": maps,\n    }\n    stage = {\n        \"material\": material,\n        \"uv_scale\": [random.uniform(0.8, 1.2), random.uniform(0.8, 1.2)],\n        \"uv_rotation\": random.uniform(0, 360),\n    }\n    return stage\n</code></pre>"},{"location":"combiner/#simian.combiner.generate_stage_captions","title":"<code>generate_stage_captions(combination)</code>","text":"<p>Generate captions for the stage based on the combination data.</p> <p>Parameters:</p> Name Type Description Default <code>combination</code> <code>Dict[str, Any]</code> <p>Combination data.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of stage captions.</p> Source code in <code>simian/combiner.py</code> <pre><code>def generate_stage_captions(combination: Dict[str, Any]) -&gt; List[str]:\n    \"\"\"\n    Generate captions for the stage based on the combination data.\n\n    Args:\n        combination (Dict[str, Any]): Combination data.\n\n    Returns:\n        List[str]: List of stage captions.\n    \"\"\"\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n\n    stage_data_path = os.path.join(current_dir, \"../data/stage_data.json\")\n    stage_data = read_json_file(stage_data_path)\n\n    background_prefix = random.choice(stage_data[\"background_names\"])\n    floor_prefix = random.choice(stage_data[\"material_names\"])\n    background_name = combination[\"background\"][\"name\"]\n    floor_material_name = combination[\"stage\"][\"material\"][\"name\"]\n\n    # remove all numbers and trim\n    floor_material_name = \"\".join(\n        [i for i in floor_material_name if not i.isdigit()]\n    ).strip()\n    background_name = (\n        \"\".join([i for i in background_name if not i.isdigit()])\n        .strip()\n        .replace(\"#\", \"\")\n        .replace(\"_\", \" \")\n        .replace(\"(\", \"\")\n        .replace(\")\", \"\")\n    )\n    caption_parts = [\n        f\"The {background_prefix} is {background_name}.\",\n        f\"The {floor_prefix} is {floor_material_name}.\",\n    ]\n    return caption_parts\n</code></pre>"},{"location":"combiner/#simian.combiner.meters_to_feet_rounded","title":"<code>meters_to_feet_rounded(meters)</code>","text":"<p>Convert meters to feet and round the result.</p> <p>Parameters:</p> Name Type Description Default <code>meters</code> <code>float</code> <p>Distance in meters.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Distance in feet (rounded).</p> Source code in <code>simian/combiner.py</code> <pre><code>def meters_to_feet_rounded(meters: float) -&gt; int:\n    \"\"\"\n    Convert meters to feet and round the result.\n\n    Args:\n        meters (float): Distance in meters.\n\n    Returns:\n        int: Distance in feet (rounded).\n    \"\"\"\n    feet_per_meter = 3.28084\n    return round(meters * feet_per_meter)\n</code></pre>"},{"location":"combiner/#simian.combiner.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse command line arguments using argparse.</p> <p>Returns:</p> Type Description <code>Namespace</code> <p>argparse.Namespace: Parsed arguments.</p> Source code in <code>simian/combiner.py</code> <pre><code>def parse_args() -&gt; argparse.Namespace:\n    \"\"\"\n    Parse command line arguments using argparse.\n\n    Returns:\n        argparse.Namespace: Parsed arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Generate random camera combinations.\")\n    parser.add_argument(\n        \"--count\", type=int, default=10, help=\"Number of combinations to generate\"\n    )\n    parser.add_argument(\n        \"--seed\", type=int, default=None, help=\"Seed for the random number generator\"\n    )\n    parser.add_argument(\n        \"--max_number_of_objects\",\n        type=int,\n        default=5,\n        help=\"Maximum number of objects to randomly select\",\n    )\n    parser.add_argument(\n        \"--camera_file_path\",\n        type=str,\n        default=\"data/camera_data.json\",\n        help=\"Path to the JSON file containing camera data\",\n    )\n    parser.add_argument(\n        \"--object_data_path\",\n        type=str,\n        default=\"data/object_data.json\",\n        help=\"Path to the JSON file containing object data\",\n    )\n    parser.add_argument(\n        \"--texture_data_path\",\n        type=str,\n        default=\"datasets/texture_data.json\",\n        help=\"Path to the JSON file containing texture data\",\n    )\n    parser.add_argument(\n        \"--datasets_path\",\n        type=str,\n        default=\"data/datasets.json\",\n        help=\"Path to the file which lists all the datasets to use\",\n    )\n    parser.add_argument(\n        \"--cap3d_captions_path\",\n        type=str,\n        default=\"datasets/cap3d_captions.json\",\n        help=\"Path to the JSON file containing captions data\",\n    )\n    parser.add_argument(\n        \"--simdata_path\",\n        type=str,\n        default=\"datasets\",\n        help=\"Path to the simdata directory\",\n    )\n    parser.add_argument(\n        \"--output_path\",\n        type=str,\n        default=\"combinations.json\",\n        help=\"Path to the output file\",\n    )\n    parser.add_argument(\n        \"--stage_data_path\",\n        type=str,\n        default=\"data/stage_data.json\",\n        help=\"Path to the JSON file containing stage data\",\n    )\n    parser.add_argument(\n        \"--movement\",\n        action=\"store_true\",\n        help=\"Apply movement to all, none, or random objects.\"\n    )\n    parser.add_argument(\n        \"--ontop\",\n        action=\"store_true\",\n        help=\"Allow objects to be on top of each other.\"\n    )\n    parser.add_argument(\n        \"--camera_follow\",\n        action=\"store_true\",\n        help=\"Camera will follow specified object as it moves (for individual objects).\"\n    )\n    parser.add_argument(\n        \"--random\",\n        action=\"store_true\",\n        help=\"Randomly apply movement, object stacking, and camera follow effects\"\n    )\n    return parser.parse_args()\n</code></pre>"},{"location":"combiner/#simian.combiner.read_json_file","title":"<code>read_json_file(file_path)</code>","text":"<p>Read a JSON file and return the data as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the JSON file.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Data from the JSON file.</p> Source code in <code>simian/combiner.py</code> <pre><code>def read_json_file(file_path: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Read a JSON file and return the data as a dictionary.\n\n    Args:\n        file_path (str): Path to the JSON file.\n\n    Returns:\n        Dict[str, Any]: Data from the JSON file.\n    \"\"\"\n    with open(file_path, \"r\") as file:\n        return json.load(file)\n</code></pre>"},{"location":"combiner/#simian.combiner.speed_factor_to_percentage","title":"<code>speed_factor_to_percentage(speed_factor)</code>","text":"<p>Convert speed factor to a percentage string. Copy codeArgs:     speed_factor (float): Speed factor value.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Speed factor as a percentage string.</p> Source code in <code>simian/combiner.py</code> <pre><code>def speed_factor_to_percentage(speed_factor: float) -&gt; str:\n    \"\"\"\n    Convert speed factor to a percentage string.\n    Copy codeArgs:\n        speed_factor (float): Speed factor value.\n\n    Returns:\n        str: Speed factor as a percentage string.\n    \"\"\"\n    rounded_speed_factor = round(speed_factor, 2)\n    percentage = int(rounded_speed_factor * 100)\n    return f\"{percentage}%\"\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Below are some quick notes to get you up and running. Please read through the rest of the documentation for more detailed information.</p>"},{"location":"getting_started/#setup","title":"\ud83d\udda5\ufe0f Setup","text":"<p>NOTE: Simian requires Python 3.11.</p> <ol> <li>Install dependences:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Download the datasets:</li> </ol> <pre><code>./scripts/data/get_data.sh\n</code></pre> <ol> <li>[OPTIONAL] If you're on a headless Linux server, install Xorg and start it:</li> </ol> <pre><code>sudo apt-get install xserver-xorg -y &amp;&amp; \\\nsudo python3 scripts/start_x_server.py start\n</code></pre>"},{"location":"getting_started/#usage","title":"\ud83d\udcf8 Usage","text":""},{"location":"getting_started/#generating-combinations","title":"Generating Combinations","text":"<pre><code>python3 -m simian.combiner --count 1000 --seed 42\n</code></pre>"},{"location":"getting_started/#generating-videos","title":"Generating Videos","text":"<p>To generate a video(s): </p> <pre><code>python3 -m simian.batch --start_index 0 --end_index 1000 --width 1024 --height 576 --start_frame 1 --end_frame 65\n</code></pre> <p>To generate an image(s):</p> <pre><code>python3 -m simian.batch --start_index 0 --end_index 1000 --width 1024 --height 576 --start_frame 1 --end_frame 65\n</code></pre> <p>You can generate individually:</p> <pre><code># MacOS\npython -m simian.render\n\n# Linux\npython -m simian.render\n\n## Kitchen sink\npython -m simian.render -- --width 1920 --height 1080 --combination_index 0 --output_dir ./renders --hdri_path ./backgrounds --start_frame 1 --end_frame 65\n</code></pre> <p>Configure the flags as needed: - <code>--width</code> and <code>--height</code> are the resolution of the video. - <code>--combination_index</code> is the index of the combination to render. - <code>--output_dir</code> is the directory to save the rendered video. - <code>--hdri_path</code> is the directory containing the background images. - <code>--start_frame</code> and <code>--end_frame</code> are the start and end frames of the video.</p> <p>Or generate all or part of the combination set using the <code>batch.py</code> script:</p> <pre><code>python3 -m simian.batch --start_index 0 --end_index 1000 --width 1024 --height 576 --start_frame 1 --end_frame 65\n</code></pre>"},{"location":"getting_started/#clean-up-captions","title":"Clean up Captions","text":"<p>Make captions more prompt friendly.</p> <p>NOTE: Create a .env file and add your OpenAI API key</p> <pre><code>python3 scripts/rewrite_captions.py\n</code></pre>"},{"location":"getting_started/#distributed-rendering","title":"Distributed rendering","text":"<p>Rendering can be distributed across multiple machines using the \"simian.py\" and \"worker.py\" scripts.</p> <p>You will need to set up Redis and obtain Huggingface API key to use this feature.</p>"},{"location":"getting_started/#set-up-redis","title":"Set Up Redis","text":"<p>You can make a free Redis account here.</p> <p>For local testing and multiple local workers, you can use the following script to set up a local instance of Redis:</p> <pre><code>scripts/setup_redis.sh\n</code></pre>"},{"location":"getting_started/#huggingface-api-key","title":"Huggingface API Key","text":"<p>You can get a Huggingface API key here.</p> <p>Now, start your workers</p> <pre><code>export REDIS_HOST=&lt;myhost&gt;.com\nexport REDIS_PORT=1337\nexport REDIS_USER=default\nexport REDIS_PASSWORD=&lt;somepassword&gt;\nexport HF_TOKEN=&lt;token&gt;\nexport HF_REPO_ID=&lt;repo_id&gt;\ncelery -A simian.worker worker --loglevel=info\n</code></pre> <p>You can also build and run the worker with Docker</p> <pre><code># build the container\ndocker build -t simian-worker .\n\n# run the container with .env\ndocker run --env-file .env simian-worker\n\n# run the container with environment variables\ndocker run -e REDIS_HOST={myhost} -e REDIS_PORT={port} -e REDIS_USER=default -e REDIS_PASSWORD={some password} -e HF_TOKEN={token} -e HF_REPO_ID={repo_id} simian-worker\n</code></pre> <p>Finally, issue work to your task queue</p> <pre><code>python3 -m simian.distributed --width 1024 --height 576\n</code></pre> <p>If you want to use a custom or hosted Redis instance (recommended), you can add the redis details like this:</p> <pre><code>export REDIS_HOST=&lt;myhost&gt;.com\nexport REDIS_PORT=1337\nexport REDIS_USER=default\nexport REDIS_PASSWORD=&lt;somepassword&gt;\n</code></pre> <p>To run tests look into the test folder and run whichever test file you want</p> <pre><code>python object_test.py\n</code></pre>"},{"location":"object/","title":"Object","text":"<p>The <code>object</code> module contains classes and functions for managing objects in the scene. It handles loading objects from datasources, setting their position, scale, and orientation, as well as how materials and textures are handled.</p> <p>Objects are loaded from datasources, such as Objaverse. The <code>object</code> module is responsible for managing the objects that are loaded into the scene. It handles loading the objects, setting their position, scale and orientation, as well as how materials and textures are handled.</p>"},{"location":"object/#simian.object.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":"<p>A dictionary mapping file extensions to Blender's import functions. This allows for dynamic selection of the appropriate import function based on the file type. Each key is a string representing the file extension, and the value is a callable Blender function.</p> <p>Supported file types include: - \"obj\": Imports Wavefront OBJ files. - \"glb\": Imports GLB (binary glTF) files. - \"gltf\": Imports glTF files. - \"usd\": Imports Universal Scene Description (USD) files. - \"fbx\": Imports Autodesk FBX files. - \"stl\": Imports Stereolithography (STL) files. - \"usda\": Imports ASCII format of Universal Scene Description files. - \"dae\": Imports Collada (DAE) files. - \"ply\": Imports Polygon File Format (PLY) files. - \"abc\": Imports Alembic files. - \"blend\": Appends or links from a Blender (.blend) file.</p>"},{"location":"object/#simian.object.apply_all_modifiers","title":"<code>apply_all_modifiers(obj)</code>","text":"<p>Recursively apply all modifiers to the object and its children.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Object</code> <p>The root object.</p> required Return <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def apply_all_modifiers(obj: bpy.types.Object):\n    \"\"\"\n    Recursively apply all modifiers to the object and its children.\n\n    Args:\n        obj (bpy.types.Object): The root object.\n\n    Return:\n        None\n    \"\"\"\n\n    # Traverse the hierarchy and apply all modifiers\n    def recurse(obj):\n        # Set the active object to the current object\n        bpy.context.view_layer.objects.active = obj\n        # Check if the object has modifiers\n        if obj.modifiers:\n            # Apply each modifier\n            for modifier in obj.modifiers:\n                bpy.ops.object.modifier_apply(modifier=modifier.name)\n\n    # Recursively apply modifiers to children\n    for child in obj.children:\n        recurse(child)\n\n    # Apply modifiers to the root object itself\n    recurse(obj)\n</code></pre>"},{"location":"object/#simian.object.apply_and_remove_armatures","title":"<code>apply_and_remove_armatures()</code>","text":"<p>Apply armature modifiers to meshes and remove armature objects.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def apply_and_remove_armatures():\n    \"\"\"\n    Apply armature modifiers to meshes and remove armature objects.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n\n    # Ensure context is correct\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n    bpy.ops.object.select_all(action=\"DESELECT\")\n\n    # Iterate over all objects in the scene\n    for obj in bpy.data.objects:\n        # Check if the object is a mesh with an armature modifier\n        if obj.type == \"MESH\":\n            for modifier in obj.modifiers:\n                if modifier.type == \"ARMATURE\" and modifier.object is not None:\n                    try:\n                        remove_blendshapes_from_hierarchy(obj)\n                    except:\n                        logger.info(\"Error removing blendshapes from object.\")\n                        # write a log file with the error\n                        with open(\"error_log.txt\", \"a\") as f:\n                            f.write(\"Error removing blendshapes from object.\\n\")\n                    # Select and make the mesh active\n                    bpy.context.view_layer.objects.active = obj\n                    obj.select_set(True)\n\n                    # Apply the armature modifier\n                    bpy.ops.object.modifier_apply(modifier=modifier.name)\n\n                    # Deselect everything to clean up for the next iteration\n                    bpy.ops.object.select_all(action=\"DESELECT\")\n</code></pre>"},{"location":"object/#simian.object.delete_all_empties","title":"<code>delete_all_empties()</code>","text":"<p>Deletes all empty objects in the scene.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def delete_all_empties() -&gt; None:\n    \"\"\"Deletes all empty objects in the scene.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    for obj in bpy.data.objects:\n        if obj.type == \"EMPTY\":\n            # if the object is hidden from selection, ignore\n            if obj.hide_select:\n                continue\n            bpy.data.objects.remove(obj, do_unlink=True)\n</code></pre>"},{"location":"object/#simian.object.delete_invisible_objects","title":"<code>delete_invisible_objects()</code>","text":"<p>Deletes all invisible objects in the scene.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def delete_invisible_objects() -&gt; None:\n    \"\"\"\n    Deletes all invisible objects in the scene.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    bpy.ops.object.select_all(action=\"DESELECT\")\n    for obj in bpy.context.scene.objects:\n        if obj.hide_select:\n            continue\n        if obj.hide_viewport or obj.hide_render:\n            obj.hide_viewport = False\n            obj.hide_render = False\n            obj.hide_select = False\n            obj.select_set(True)\n    bpy.ops.object.delete()\n\n    # Delete invisible collections\n    invisible_collections = [col for col in bpy.data.collections if col.hide_viewport]\n    for col in invisible_collections:\n        bpy.data.collections.remove(col)\n</code></pre>"},{"location":"object/#simian.object.get_hierarchy_bbox","title":"<code>get_hierarchy_bbox(obj)</code>","text":"<p>Calculate the bounding box of an object and its children.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Object</code> <p>The root object.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[float, float]</code> <p>A tuple containing the minimum and maximum coordinates of the bounding box.</p> Source code in <code>simian/object.py</code> <pre><code>def get_hierarchy_bbox(obj) -&gt; tuple[float, float]:\n    \"\"\"\n    Calculate the bounding box of an object and its children.\n\n    Args:\n        obj (bpy.types.Object): The root object.\n\n    Returns:\n        tuple: A tuple containing the minimum and maximum coordinates of the bounding box.\n    \"\"\"\n    # Ensure the object's matrix_world is updated\n    bpy.context.view_layer.update()\n\n    # Initialize min and max coordinates with extremely large and small values\n    min_coord = [float(\"inf\"), float(\"inf\"), float(\"inf\")]\n    max_coord = [-float(\"inf\"), -float(\"inf\"), -float(\"inf\")]\n\n    # Function to update min and max coordinates\n    def update_bounds(obj: bpy.types.Object) -&gt; None:\n        \"\"\"\n        Update the minimum and maximum coordinates based on the object's bounding box.\n\n        Args:\n            obj (bpy.types.Object): The object whose bounding box is to be updated.\n\n        Returns:\n            None\n        \"\"\"\n        # Update the object's bounding box based on its world matrix\n        bbox_corners = [obj.matrix_world @ Vector(corner) for corner in obj.bound_box]\n        for corner in bbox_corners:\n            for i in range(3):\n                min_coord[i] = min(min_coord[i], corner[i])\n                max_coord[i] = max(max_coord[i], corner[i])\n\n    # Recursive function to process each object\n    def process_object(obj: bpy.types.Object) -&gt; None:\n        \"\"\"\n        Recursively process each object and update bounding box coordinates.\n\n        Args:\n            obj (bpy.types.Object): The object to be processed.\n\n        Returns:\n            None\n        \"\"\"\n        update_bounds(obj)\n        for child in obj.children:\n            process_object(child)\n\n    # Start processing from the root object\n    process_object(obj)\n\n    return min_coord, max_coord\n</code></pre>"},{"location":"object/#simian.object.get_meshes_in_hierarchy","title":"<code>get_meshes_in_hierarchy(obj)</code>","text":"<p>Recursively collects all mesh objects in the hierarchy starting from the given object.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Object</code> <p>The root object from which to start collecting mesh objects.</p> required <p>Returns:</p> Type Description <code>List[Object]</code> <p>List[bpy.types.Object]: A list of mesh objects found in the hierarchy.</p> Source code in <code>simian/object.py</code> <pre><code>def get_meshes_in_hierarchy(obj: bpy.types.Object) -&gt; List[bpy.types.Object]:\n    \"\"\"\n    Recursively collects all mesh objects in the hierarchy starting from the given object.\n\n    Args:\n        obj (bpy.types.Object): The root object from which to start collecting mesh objects.\n\n    Returns:\n        List[bpy.types.Object]: A list of mesh objects found in the hierarchy.\n    \"\"\"\n    # Initialize an empty list to store mesh objects\n    meshes = []\n\n    # Check if the current object is a mesh and add it to the list if it is\n    if obj.type == \"MESH\":\n        meshes.append(obj)\n\n    # Initialize an empty list to store mesh objects from child objects\n    new_meshes = []\n\n    # Recursively call the function for each child object and collect their meshes\n    for child in obj.children:\n        # Add meshes from child objects to the list\n        new_meshes += get_meshes_in_hierarchy(child)\n\n    # Combine meshes from the current object and its children and return the list\n    return meshes + new_meshes\n</code></pre>"},{"location":"object/#simian.object.get_terrain_height","title":"<code>get_terrain_height(location)</code>","text":"<p>Get the height of the terrain at a specific location. Args:     location (Vector): The location to get the height for. Returns:     float: The height of the terrain at the specified location.</p> Source code in <code>simian/object.py</code> <pre><code>def get_terrain_height(location: Vector) -&gt; float:\n    \"\"\"\n    Get the height of the terrain at a specific location.\n    Args:\n        location (Vector): The location to get the height for.\n    Returns:\n        float: The height of the terrain at the specified location.\n    \"\"\"\n    bpy.context.view_layer.update()\n    ray_origin = Vector((location.x, location.y, 25))  # Ray origin below the location\n    ray_direction = Vector((0, 0, -1))  # Ray direction downwards\n\n    depsgraph = bpy.context.evaluated_depsgraph_get()\n    hit_objects = ['OpaqueTerrain', 'OpaqueTerrain_fine']\n    terrain_objects = [obj for obj in bpy.context.scene.objects if obj.name in hit_objects]\n\n    result = False\n    hit_location = None\n\n    for obj in terrain_objects:\n        if obj.hide_get():\n            continue\n\n        result_local, hit_location_local, normal, index, hit_obj, matrix = bpy.context.scene.ray_cast(\n            depsgraph, ray_origin, ray_direction)\n\n        if result_local and hit_obj.name in hit_objects:\n            if hit_obj.name == 'OpaqueTerrain_fine':\n                result = result_local\n                hit_location = hit_location_local\n                break\n            if hit_obj.name == 'OpaqueTerrain' and not result:\n                result = result_local\n                hit_location = hit_location_local\n\n    if result:\n        return hit_location.z\n    else:\n        # logger.info(\"Ray did not hit any terrain\")\n        return 0.0  # Default to 0 if no intersection is found\n</code></pre>"},{"location":"object/#simian.object.join_objects_in_hierarchy","title":"<code>join_objects_in_hierarchy(obj)</code>","text":"<p>Joins a list of objects into a single object.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>List[Object]</code> <p>List of objects to join.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def join_objects_in_hierarchy(obj: bpy.types.Object) -&gt; None:\n    \"\"\"\n    Joins a list of objects into a single object.\n\n    Args:\n        objects (List[bpy.types.Object]): List of objects to join.\n\n    Returns:\n        None\n    \"\"\"\n\n    meshes = get_meshes_in_hierarchy(obj)\n\n    # Select and activate meshes\n    bpy.ops.object.select_all(action=\"DESELECT\")\n    for mesh in meshes:\n        mesh.select_set(True)\n\n    # Set the last selected mesh as active and check if it's valid for mode setting\n    if meshes:\n        bpy.context.view_layer.objects.active = meshes[0]\n        if (\n            bpy.context.view_layer.objects.active is not None\n            and bpy.context.view_layer.objects.active.type == \"MESH\"\n        ):\n            # Use Context.temp_override() to create a temporary context override\n            with bpy.context.temp_override(\n                active_object=bpy.context.view_layer.objects.active,\n                selected_objects=meshes,\n            ):\n                # Set the object mode to 'OBJECT' using the operator with the temporary context override\n                bpy.ops.object.mode_set(mode=\"OBJECT\")\n\n                # Join meshes using the bpy.ops.object.join() operator with a custom context override\n                if len(meshes) &gt; 1:\n                    bpy.ops.object.join()\n                else:\n                    logger.info(\"Not enough meshes to join.\")\n        else:\n            logger.info(\"Active object is not a valid mesh.\")\n    else:\n        logger.info(\"No meshes found to set as active.\")\n</code></pre>"},{"location":"object/#simian.object.load_object","title":"<code>load_object(object_path)</code>","text":"<p>Loads a model with a supported file extension into the scene.</p> <p>Parameters:</p> Name Type Description Default <code>object_path</code> <code>str</code> <p>Path to the model file.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file extension is not supported.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def load_object(object_path: str) -&gt; None:\n    \"\"\"\n    Loads a model with a supported file extension into the scene.\n\n    Args:\n        object_path (str): Path to the model file.\n\n    Raises:\n        ValueError: If the file extension is not supported.\n\n    Returns:\n        None\n    \"\"\"\n    file_extension = object_path.split(\".\")[-1].lower()\n    if file_extension is None:\n        raise ValueError(f\"Unsupported file type: {object_path}\")\n\n    # load from existing import functions\n    import_function = IMPORT_FUNCTIONS[file_extension]\n\n    if file_extension == \"blend\":\n        import_function(directory=object_path, link=False)\n    elif file_extension in {\"glb\", \"gltf\"}:\n        import_function(filepath=object_path, merge_vertices=True)\n    else:\n        import_function(filepath=object_path)\n</code></pre>"},{"location":"object/#simian.object.lock_all_objects","title":"<code>lock_all_objects()</code>","text":"<p>Locks all objects in the scene from selection and returns a list of these objects.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def lock_all_objects() -&gt; None:\n    \"\"\"\n    Locks all objects in the scene from selection and returns a list of these objects.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    locked_objects = []\n    for obj in bpy.context.scene.objects:\n        obj.hide_select = True\n        locked_objects.append(obj)\n    return locked_objects\n</code></pre>"},{"location":"object/#simian.object.normalize_object_scale","title":"<code>normalize_object_scale(obj, scale_factor=1.0)</code>","text":"<p>Scales the object by a factor.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Object</code> <p>The object to scale.</p> required <code>scale_factor</code> <code>float</code> <p>The factor to scale the object by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Object</code> <p>bpy.types.Object: The scaled object.</p> Source code in <code>simian/object.py</code> <pre><code>def normalize_object_scale(\n    obj: bpy.types.Object, scale_factor: float = 1.0\n) -&gt; bpy.types.Object:\n    \"\"\"\n    Scales the object by a factor.\n\n    Args:\n        obj (bpy.types.Object): The object to scale.\n        scale_factor (float, optional): The factor to scale the object by. Defaults to 1.0.\n\n    Returns:\n        bpy.types.Object: The scaled object.\n    \"\"\"\n    # Get the bounding box of the object and its children\n    bbox_min, bbox_max = get_hierarchy_bbox(obj)\n\n    # Calculate the scale of the bounding box and scale the object if necessary\n    bbox_dimensions = [bbox_max[i] - bbox_min[i] for i in range(3)]\n    max_dimension = max(bbox_dimensions)\n    scale = scale_factor / max_dimension\n    obj.scale = (scale, scale, scale)\n    # make sure object is active and apply the scale\n    bpy.context.view_layer.objects.active = obj\n    bpy.ops.object.transform_apply(location=False, rotation=False, scale=True)\n    return obj\n</code></pre>"},{"location":"object/#simian.object.optimize_meshes_in_hierarchy","title":"<code>optimize_meshes_in_hierarchy(obj)</code>","text":"<p>Recursively optimize meshes in the hierarchy by removing doubles and setting materials to double-sided.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Object</code> <p>The root object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def optimize_meshes_in_hierarchy(obj: bpy.types.Object) -&gt; None:\n    \"\"\"\n    Recursively optimize meshes in the hierarchy by removing doubles and setting materials to double-sided.\n\n    Args:\n        obj (bpy.types.Object): The root object.\n\n    Returns:\n        None\n    \"\"\"\n\n    if obj.type == \"MESH\":\n        # Go into edit mode and select the mesh\n        bpy.context.view_layer.objects.active = obj\n\n        # Go to edit mode\n        bpy.ops.object.mode_set(mode=\"EDIT\")\n\n        # Select all verts\n        bpy.ops.mesh.select_all(action=\"SELECT\")\n\n        bpy.ops.mesh.remove_doubles(threshold=0.0001)\n        angle_limit = 0.000174533 * 10\n\n        # Go to lines mode\n        bpy.ops.mesh.select_mode(type=\"EDGE\")\n\n        # Deselect all\n        bpy.ops.mesh.select_all(action=\"DESELECT\")\n\n        # Select all edges\n        bpy.ops.mesh.select_all(action=\"SELECT\")\n\n        # bpy.ops.mesh.dissolve_limited(angle_limit=angle_limit, use_dissolve_boundaries=True, delimit={'NORMAL', 'MATERIAL', 'SEAM', 'SHARP', 'UV'})\n\n        # Set all materials to be double sided\n        for slot in obj.material_slots:\n            # slot.material.use_backface_culling = False\n            if slot.material.blend_method == \"BLEND\":\n                slot.material.blend_method = \"HASHED\"\n\n        # Return to object mode\n        bpy.ops.object.mode_set(mode=\"OBJECT\")\n\n    for child in obj.children:\n        optimize_meshes_in_hierarchy(child)\n</code></pre>"},{"location":"object/#simian.object.remove_blendshapes_from_hierarchy","title":"<code>remove_blendshapes_from_hierarchy(obj)</code>","text":"<p>Recursively removes blendshapes from all models in the hierarchy under the given object.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Object</code> <p>The root object of the hierarchy.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def remove_blendshapes_from_hierarchy(obj: bpy.types.Object) -&gt; None:\n    \"\"\"\n    Recursively removes blendshapes from all models in the hierarchy under the given object.\n\n    Args:\n        obj (bpy.types.Object): The root object of the hierarchy.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure context is correct\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n    bpy.ops.object.select_all(action=\"DESELECT\")\n\n    def remove_blendshapes(obj):\n        if obj.type == \"MESH\":\n            # Select and make the mesh active\n            bpy.context.view_layer.objects.active = obj\n            obj.select_set(True)\n\n            # Remove blendshapes\n            if obj.data.shape_keys:\n                obj.shape_key_clear()\n\n    # Traverse the hierarchy and remove blendshapes\n    def traverse_hierarchy(obj):\n        remove_blendshapes(obj)\n        for child in obj.children:\n            traverse_hierarchy(child)\n\n    # Start traversing from the given object\n    traverse_hierarchy(obj)\n\n    # Deselect everything to clean up\n    bpy.ops.object.select_all(action=\"DESELECT\")\n</code></pre>"},{"location":"object/#simian.object.remove_small_geometry","title":"<code>remove_small_geometry(obj, min_vertex_count=10)</code>","text":"<p>Remove free-hanging geometry with fewer vertices than specified by separating by loose parts, deleting small ones, and re-joining them.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Object</code> <p>The object to process.</p> required <code>min_vertex_count</code> <code>int</code> <p>Minimum number of vertices required to keep a part of the mesh. Default is 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>Optional[Object]</code> <p>bpy.types.Object or None: The processed object if successful, None otherwise.</p> Source code in <code>simian/object.py</code> <pre><code>def remove_small_geometry(\n    obj: bpy.types.Object, min_vertex_count: int = 10\n) -&gt; Optional[bpy.types.Object]:\n    \"\"\"\n    Remove free-hanging geometry with fewer vertices than specified by separating by loose parts,\n    deleting small ones, and re-joining them.\n\n    Args:\n        obj (bpy.types.Object): The object to process.\n        min_vertex_count (int, optional): Minimum number of vertices required to keep a part of the mesh. Default is 10.\n\n    Returns:\n        bpy.types.Object or None: The processed object if successful, None otherwise.\n    \"\"\"\n    # Ensure the object is a mesh\n    if obj is not None and obj.type != \"MESH\":\n        logger.info(\"Object is not a mesh.\")\n        return None\n\n    # Make sure the object is active and we're in object mode\n    bpy.context.view_layer.objects.active = obj\n    obj.select_set(True)\n    # select all children\n    for child in obj.children:\n        child.select_set(True)\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n\n    # Separate by loose parts\n    bpy.ops.mesh.separate(type=\"LOOSE\")\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n\n    # Deselect all to start clean\n    bpy.ops.object.select_all(action=\"DESELECT\")\n\n    # Iterate over all new objects created by the separate operation\n    for ob in bpy.context.selected_objects:\n        # Re-select the object to make it active\n        bpy.context.view_layer.objects.active = ob\n        ob.select_set(True)\n\n        # Check vertex count\n        if len(ob.data.vertices) &lt; min_vertex_count:\n            # Delete the object if it doesn't meet the vertex count requirement\n            bpy.ops.object.delete()\n\n    # Optionally, re-join remaining objects if necessary\n    bpy.ops.object.select_all(action=\"SELECT\")\n    bpy.ops.object.join()\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n\n    return obj\n</code></pre>"},{"location":"object/#simian.object.set_pivot_to_bottom","title":"<code>set_pivot_to_bottom(obj)</code>","text":"<p>Set the pivot of the object to the center of mass, and place it on the terrain surface. Args:     obj (bpy.types.Object): The object to adjust. Returns:     None</p> Source code in <code>simian/object.py</code> <pre><code>def set_pivot_to_bottom(obj: bpy.types.Object) -&gt; None:\n    \"\"\"\n    Set the pivot of the object to the center of mass, and place it on the terrain surface.\n    Args:\n        obj (bpy.types.Object): The object to adjust.\n    Returns:\n        None\n    \"\"\"\n    # Update the view layer to ensure the latest changes are considered\n    bpy.context.view_layer.update()\n\n    # Calculate the center of mass\n    center_of_mass = obj.location\n\n    # Calculate the bounding box bottom\n    bbox_min = [obj.matrix_world @ Vector(corner) for corner in obj.bound_box][0]\n    for corner in obj.bound_box:\n        world_corner = obj.matrix_world @ Vector(corner)\n        if world_corner.z &lt; bbox_min.z:\n            bbox_min = world_corner\n\n    # Set origin to the center of mass\n    bpy.ops.object.origin_set(type=\"ORIGIN_CENTER_OF_MASS\", center=\"BOUNDS\")\n\n    # Get the terrain height at the object's bounding box minimum location\n    terrain_height = get_terrain_height(bbox_min)\n    # logger.info(f\"Terrain height at object location: {terrain_height}\")\n\n    # Calculate the object's height\n    obj_height = center_of_mass.z - bbox_min.z\n\n    # Move the object to the terrain height plus its height\n    obj.location.z = terrain_height + obj_height\n\n    # Apply transformations\n    bpy.ops.object.transform_apply(location=True, rotation=True, scale=True)\n</code></pre>"},{"location":"object/#simian.object.unlock_objects","title":"<code>unlock_objects(objs)</code>","text":"<p>Unlocks a given list of objects for selection.</p> <p>Parameters:</p> Name Type Description Default <code>objs</code> <code>list</code> <p>A list of Blender objects to be unlocked.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def unlock_objects(objs: List[bpy.types.Object]) -&gt; None:\n    \"\"\"\n    Unlocks a given list of objects for selection.\n\n    Args:\n        objs (list): A list of Blender objects to be unlocked.\n\n    Returns:\n        None\n    \"\"\"\n    for obj in objs:\n        obj.hide_select = False\n</code></pre>"},{"location":"object/#simian.object.unparent_keep_transform","title":"<code>unparent_keep_transform(obj)</code>","text":"<p>Unparents an object but keeps its transform.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Object</code> <p>The object to unparent.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/object.py</code> <pre><code>def unparent_keep_transform(obj: bpy.types.Object) -&gt; None:\n    \"\"\"\n    Unparents an object but keeps its transform.\n\n    Args:\n        obj (bpy.types.Object): The object to unparent.\n\n    Returns:\n        None\n    \"\"\"\n    # clear the parent object, but keep the transform\n    bpy.ops.object.parent_clear(type=\"CLEAR_KEEP_TRANSFORM\")\n    # clear rotation and scale and apply\n    bpy.ops.object.transform_apply(location=False, rotation=True, scale=True)\n</code></pre>"},{"location":"postprocessing/","title":"Postprocessing","text":"<p>The <code>postprocessing</code> module is responsible for applying postprocessing effects to the rendered videos. It handles adding effects such as color correction, vignetting, and lens distortion to the videos.</p>"},{"location":"postprocessing/#simian.postprocessing.enable_effect","title":"<code>enable_effect(context, effect_name)</code>","text":"<p>Enables the cel shading effect in the compositor.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>The Blender context.</p> required <code>effect_name</code> <code>str</code> <p>The name of the effect to enable.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/postprocessing.py</code> <pre><code>def enable_effect(context: bpy.types.Context, effect_name: str) -&gt; None:\n    \"\"\"\n    Enables the cel shading effect in the compositor.\n\n    Args:\n        context (bpy.types.Context): The Blender context.\n        effect_name (str): The name of the effect to enable.\n\n    Returns:\n        None\n    \"\"\"\n    # Make sure compositor use is turned on\n    context.scene.render.use_compositing = True\n    context.scene.use_nodes = True\n    tree: bpy.types.NodeTree = context.scene.node_tree\n\n    # Clear existing nodes\n    for node in tree.nodes:\n        tree.nodes.remove(node)\n\n    # Enable Normal Pass and Diffuse Pass\n    # These passes provide the necessary information for shading and color\n    # Reference: https://docs.blender.org/manual/en/latest/render/layers/passes.html\n    view_layer: bpy.types.ViewLayer = context.view_layer\n    view_layer.use_pass_normal = True\n    view_layer.use_pass_diffuse_color = True\n    view_layer.use_pass_environment = True  # Enable the environment pass\n    view_layer.use_pass_z = True\n\n    # get the function for the effect\n    effect_function: Any = effects.get(effect_name)\n\n    # Set up the nodes for cel shading\n    effect_function(context)\n</code></pre>"},{"location":"postprocessing/#simian.postprocessing.setup_compositor_for_black_and_white","title":"<code>setup_compositor_for_black_and_white(context)</code>","text":"<p>Sets up the compositor for a black and white effect using Blender's Eevee engine. Assumes that the context provided is valid and that the scene uses Eevee.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>The Blender context.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/postprocessing.py</code> <pre><code>def setup_compositor_for_black_and_white(context: bpy.types.Context) -&gt; None:\n    \"\"\"\n    Sets up the compositor for a black and white effect using Blender's Eevee engine.\n    Assumes that the context provided is valid and that the scene uses Eevee.\n\n    Args:\n        context (bpy.types.Context): The Blender context.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure the use of nodes in the scene's compositing.\n    scene = context.scene\n    scene.use_nodes = True\n    tree = scene.node_tree\n\n    # Create necessary nodes\n    render_layers = tree.nodes.new(type=\"CompositorNodeRLayers\")\n    hue_sat = tree.nodes.new(type=\"CompositorNodeHueSat\")\n    composite = tree.nodes.new(\n        type=\"CompositorNodeComposite\"\n    )  # Ensure there's a composite node\n\n    # Position nodes\n    render_layers.location = (-300, 0)\n    hue_sat.location = (100, 0)\n    composite.location = (300, 0)\n\n    # Configure Hue Sat node for desaturation\n    hue_sat.inputs[\"Saturation\"].default_value = (\n        0  # Reduce saturation to zero to get grayscale\n    )\n    # increase contrast\n    hue_sat.inputs[\"Value\"].default_value = 2.0\n\n    # Link nodes\n    links = tree.links\n    links.new(render_layers.outputs[\"Image\"], hue_sat.inputs[\"Image\"])\n    links.new(\n        hue_sat.outputs[\"Image\"], composite.inputs[\"Image\"]\n    )  # Direct output to the composite node\n</code></pre>"},{"location":"postprocessing/#simian.postprocessing.setup_compositor_for_cel_shading","title":"<code>setup_compositor_for_cel_shading(context)</code>","text":"<p>Sets up the compositor for a cel shading effect using Blender's Eevee engine. The node setup is based on the theory of using normal and diffuse passes to create a stylized, non-photorealistic look with subtle transitions between shadows and highlights. Assumes the context provided is valid and that the scene uses Eevee.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>The Blender context.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/postprocessing.py</code> <pre><code>def setup_compositor_for_cel_shading(context: bpy.types.Context) -&gt; None:\n    \"\"\"\n    Sets up the compositor for a cel shading effect using Blender's Eevee engine.\n    The node setup is based on the theory of using normal and diffuse passes to create\n    a stylized, non-photorealistic look with subtle transitions between shadows and highlights.\n    Assumes the context provided is valid and that the scene uses Eevee.\n\n    Args:\n        context (bpy.types.Context): The Blender context.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure the use of nodes in the scene's compositing\n    scene: bpy.types.Scene = context.scene\n    scene.use_nodes = True\n    tree: bpy.types.NodeTree = scene.node_tree\n\n    # Create necessary nodes for compositing\n    # Render Layers Node: Provides access to the render passes\n    # Normal Node: Converts the normal pass into a usable format\n    # Color Ramp Nodes: Control the shading and highlight areas\n    # Mix RGB Nodes: Combine the shading and highlight with the diffuse color\n    # Composite Node: The final output node\n    render_layers: Any = tree.nodes.new(type=\"CompositorNodeRLayers\")\n    normal_node: Any = tree.nodes.new(type=\"CompositorNodeNormal\")\n    color_ramp_shadow: Any = tree.nodes.new(type=\"CompositorNodeValToRGB\")\n    color_ramp_highlight: Any = tree.nodes.new(type=\"CompositorNodeValToRGB\")\n    mix_rgb_shadow: Any = tree.nodes.new(type=\"CompositorNodeMixRGB\")\n    mix_rgb_highlight: Any = tree.nodes.new(type=\"CompositorNodeMixRGB\")\n    alpha_over: Any = tree.nodes.new(\n        type=\"CompositorNodeAlphaOver\"\n    )  # Add an Alpha Over node\n    composite: Any = tree.nodes.new(type=\"CompositorNodeComposite\")\n\n    # Configure Mix RGB nodes\n    # Multiply blend mode for shadows to darken the diffuse color slightly\n    # Overlay blend mode for highlights to create subtle bright highlights\n    # Reference: https://docs.blender.org/manual/en/latest/compositing/types/color/mix.html\n    mix_rgb_shadow.blend_type = \"MULTIPLY\"\n    mix_rgb_highlight.blend_type = \"OVERLAY\"\n    mix_rgb_shadow.use_clamp = True\n    mix_rgb_highlight.use_clamp = True\n\n    # Configure Shadow Color Ramp\n    color_ramp_shadow.color_ramp.interpolation = \"EASE\"\n    color_ramp_shadow.color_ramp.elements[0].position = 0.5\n    color_ramp_shadow.color_ramp.elements[1].position = 0.8\n    color_ramp_shadow.color_ramp.elements[0].color = (0.5, 0.5, 0.5, 1)  # Mid Gray\n    color_ramp_shadow.color_ramp.elements[1].color = (1, 1, 1, 1)  # White\n\n    # Configure Highlight Color Ramp\n    color_ramp_highlight.color_ramp.interpolation = \"EASE\"\n    color_ramp_highlight.color_ramp.elements[0].position = 0.8\n    color_ramp_highlight.color_ramp.elements[1].position = 0.95\n    color_ramp_highlight.color_ramp.elements[0].color = (0, 0, 0, 1)  # Black\n    color_ramp_highlight.color_ramp.elements[1].color = (1, 1, 1, 1)  # White\n\n    # Adjust the Mix RGB nodes\n    mix_rgb_shadow.blend_type = \"MULTIPLY\"\n    mix_rgb_shadow.inputs[0].default_value = 1.0  # Reduce the shadow intensity\n\n    mix_rgb_highlight.blend_type = (\n        \"SCREEN\"  # Change to 'SCREEN' for better highlight blending\n    )\n    mix_rgb_highlight.inputs[0].default_value = 0.5  # Reduce the highlight intensity\n\n    # Link nodes\n    links: Any = tree.links\n    # Connect Normal pass to Normal node for shading information\n    links.new(render_layers.outputs[\"Normal\"], normal_node.inputs[\"Normal\"])\n    # Connect Normal node to Shadow Color Ramp for shadow intensity control\n    links.new(normal_node.outputs[\"Dot\"], color_ramp_shadow.inputs[\"Fac\"])\n    # Connect Normal node to Highlight Color Ramp for highlight intensity control\n    links.new(normal_node.outputs[\"Dot\"], color_ramp_highlight.inputs[\"Fac\"])\n    # Connect Shadow Color Ramp to Mix RGB Shadow node for shadow color blending\n    links.new(color_ramp_shadow.outputs[\"Image\"], mix_rgb_shadow.inputs[2])\n    # Connect Diffuse pass to Mix RGB Shadow node as the base color\n    links.new(render_layers.outputs[\"Image\"], mix_rgb_shadow.inputs[1])\n    # Connect Mix RGB Shadow to Mix RGB Highlight for combining shadows with the base color\n    links.new(mix_rgb_shadow.outputs[\"Image\"], mix_rgb_highlight.inputs[1])\n    # Connect Highlight Color Ramp to Mix RGB Highlight for adding highlights\n    links.new(color_ramp_highlight.outputs[\"Image\"], mix_rgb_highlight.inputs[2])\n    # Connect Mix RGB Highlight to Composite node for final output\n    links.new(\n        mix_rgb_highlight.outputs[\"Image\"], alpha_over.inputs[2]\n    )  # Plug the cel-shaded result into the foreground input of Alpha Over\n    links.new(\n        render_layers.outputs[\"Env\"], alpha_over.inputs[1]\n    )  # Plug the environment pass into the background input of Alpha Over\n    links.new(\n        alpha_over.outputs[\"Image\"], composite.inputs[\"Image\"]\n    )  # Plug the Alpha Over output into the Composite node\n</code></pre>"},{"location":"postprocessing/#simian.postprocessing.setup_compositor_for_depth","title":"<code>setup_compositor_for_depth(context)</code>","text":"<p>Sets up the compositor for rendering a depth map using Blender's Eevee engine. Assumes that the context provided is valid and that the scene uses Eevee.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Context</code> <p>The Blender context.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/postprocessing.py</code> <pre><code>def setup_compositor_for_depth(context: bpy.types.Context) -&gt; None:\n    \"\"\"\n    Sets up the compositor for rendering a depth map using Blender's Eevee engine.\n    Assumes that the context provided is valid and that the scene uses Eevee.\n\n    Args:\n        context (bpy.types.Context): The Blender context.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure the use of nodes in the scene's compositing.\n    scene: bpy.types.Scene = context.scene\n    scene.use_nodes = True\n    tree: bpy.types.NodeTree = scene.node_tree\n    # Create necessary nodes\n    render_layers: Any = tree.nodes.new(type=\"CompositorNodeRLayers\")\n    normalize: Any = tree.nodes.new(type=\"CompositorNodeNormalize\")\n    composite: Any = tree.nodes.new(\n        type=\"CompositorNodeComposite\"\n    )  # Ensure there's a composite node\n\n    # Position nodes\n    render_layers.location = (-300, 0)\n    normalize.location = (0, 0)\n    composite.location = (300, 0)\n\n    # Enable the Depth pass in the View Layer properties\n    view_layer: bpy.types.ViewLayer = context.view_layer\n    view_layer.use_pass_z = True\n\n    # Link nodes\n    links: Any = tree.links\n    links.new(render_layers.outputs[\"Depth\"], normalize.inputs[\"Value\"])\n    links.new(\n        normalize.outputs[\"Value\"], composite.inputs[\"Image\"]\n    )  # Direct output to the composite node\n</code></pre>"},{"location":"rendering/","title":"Rendering","text":"<p>Scene assembly and generation from individual combinations is handled by the <code>render</code> module. This module is responsible for creating the scene, setting up the camera, and rendering the scene. Most of the other modules are imported into this module.</p>"},{"location":"scene/","title":"Scene","text":"<p>The <code>scene</code> module is responsible for managing the scene in the Simian application. It handles creating the scene, setting up the camera, and rendering the scene. Most of the other modules are imported into this module.</p>"},{"location":"transform/","title":"Transform","text":"<p>The <code>transform</code> module is responsible for applying transformations to the assets in the scene. It handles scaling, rotating, and translating the assets to create the desired effect.</p>"},{"location":"transform/#simian.transform.adjust_positions","title":"<code>adjust_positions(objects, camera_yaw)</code>","text":"<p>Adjust the positions of objects based on the camera yaw.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>List[Dict]</code> <p>List of object dictionaries.</p> required <code>camera_yaw</code> <code>float</code> <p>Camera yaw angle in degrees.</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of object dictionaries with adjusted positions.</p> Source code in <code>simian/transform.py</code> <pre><code>def adjust_positions(objects: List[Dict], camera_yaw: float) -&gt; List[Dict]:\n    \"\"\"Adjust the positions of objects based on the camera yaw.\n\n    Args:\n        objects (List[Dict]): List of object dictionaries.\n        camera_yaw (float): Camera yaw angle in degrees.\n\n    Returns:\n        List[Dict]: List of object dictionaries with adjusted positions.\n    \"\"\"\n    rotation_matrix = compute_rotation_matrix(radians(camera_yaw))\n    lookup_table = {\n        0: (-1, 1),\n        1: (0, 1),\n        2: (1, 1),\n        3: (-1, 0),\n        4: (0, 0),\n        5: (1, 0),\n        6: (-1, -1),\n        7: (0, -1),\n        8: (1, -1),\n    }\n\n    empty_objs = []\n    for obj in objects:\n        grid_x, grid_y = lookup_table[obj[\"placement\"]]\n        empty_obj = obj.copy()\n        empty_obj[\"transformed_position\"] = apply_rotation(\n            [grid_x, grid_y], rotation_matrix\n        )\n        empty_objs.append(empty_obj)\n    return empty_objs\n</code></pre>"},{"location":"transform/#simian.transform.apply_rotation","title":"<code>apply_rotation(point, rotation_matrix)</code>","text":"<p>Apply rotation matrix to a point and round to integer if close.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>List[float]</code> <p>2D point as [x, y].</p> required <code>rotation_matrix</code> <code>List[List[float]]</code> <p>2D rotation matrix.</p> required <p>Returns:</p> Type Description <code>List[Union[int, float]]</code> <p>List[Union[int, float]]: Rotated point with rounded coordinates.</p> Source code in <code>simian/transform.py</code> <pre><code>def apply_rotation(\n    point: List[float], rotation_matrix: List[List[float]]\n) -&gt; List[Union[int, float]]:\n    \"\"\"Apply rotation matrix to a point and round to integer if close.\n\n    Args:\n        point (List[float]): 2D point as [x, y].\n        rotation_matrix (List[List[float]]): 2D rotation matrix.\n\n    Returns:\n        List[Union[int, float]]: Rotated point with rounded coordinates.\n    \"\"\"\n    rotated_point = np.dot(rotation_matrix, np.array(point))\n    return [\n        round(val) if abs(val - round(val)) &lt; 1e-9 else val for val in rotated_point\n    ]\n</code></pre>"},{"location":"transform/#simian.transform.check_overlap_xy","title":"<code>check_overlap_xy(bbox1, bbox2, padding=0.08)</code>","text":"<p>Check if two 2D bounding boxes overlap with optional padding.</p> <p>Parameters:</p> Name Type Description Default <code>bbox1</code> <code>List[Vector]</code> <p>First bounding box corners.</p> required <code>bbox2</code> <code>List[Vector]</code> <p>Second bounding box corners.</p> required <code>padding</code> <code>float</code> <p>Padding around the bounding boxes. Defaults to 0.08.</p> <code>0.08</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the bounding boxes overlap, False otherwise.</p> Source code in <code>simian/transform.py</code> <pre><code>def check_overlap_xy(\n    bbox1: List[Vector], bbox2: List[Vector], padding: float = 0.08\n) -&gt; bool:\n    \"\"\"Check if two 2D bounding boxes overlap with optional padding.\n\n    Args:\n        bbox1 (List[Vector]): First bounding box corners.\n        bbox2 (List[Vector]): Second bounding box corners.\n        padding (float, optional): Padding around the bounding boxes. Defaults to 0.08.\n\n    Returns:\n        bool: True if the bounding boxes overlap, False otherwise.\n    \"\"\"\n    min1 = Vector(\n        (min(corner.x for corner in bbox1), min(corner.y for corner in bbox1), 0)\n    )\n    max1 = Vector(\n        (max(corner.x for corner in bbox1), max(corner.y for corner in bbox1), 0)\n    )\n    min2 = Vector(\n        (min(corner.x for corner in bbox2), min(corner.y for corner in bbox2), 0)\n    )\n    max2 = Vector(\n        (max(corner.x for corner in bbox2), max(corner.y for corner in bbox2), 0)\n    )\n    overlap = not (\n        min1.x &gt; max2.x + padding\n        or max1.x &lt; min2.x - padding\n        or min1.y &gt; max2.y + padding\n        or max1.y &lt; min2.y - padding\n    )\n\n    # logger.info(\n    #     f\"Checking overlap between {bbox1} and {bbox2} with padding {padding}: {overlap}\"\n    # )\n    return overlap\n</code></pre>"},{"location":"transform/#simian.transform.compute_rotation_matrix","title":"<code>compute_rotation_matrix(theta)</code>","text":"<p>Compute the 2D rotation matrix for a given angle.</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>float</code> <p>Angle in radians.</p> required <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>List[List[float]]: 2D rotation matrix.</p> Source code in <code>simian/transform.py</code> <pre><code>def compute_rotation_matrix(theta: float) -&gt; List[List[float]]:\n    \"\"\"Compute the 2D rotation matrix for a given angle.\n\n    Args:\n        theta (float): Angle in radians.\n\n    Returns:\n        List[List[float]]: 2D rotation matrix.\n    \"\"\"\n    return [[cos(theta), -sin(theta)], [sin(theta), cos(theta)]]\n</code></pre>"},{"location":"transform/#simian.transform.degrees_to_radians","title":"<code>degrees_to_radians(deg)</code>","text":"<p>Convert degrees to radians.</p> <p>Parameters:</p> Name Type Description Default <code>deg</code> <code>float</code> <p>Angle in degrees.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Angle in radians.</p> Source code in <code>simian/transform.py</code> <pre><code>def degrees_to_radians(deg: float) -&gt; float:\n    \"\"\"Convert degrees to radians.\n\n    Args:\n        deg (float): Angle in degrees.\n\n    Returns:\n        float: Angle in radians.\n    \"\"\"\n    return radians(deg)\n</code></pre>"},{"location":"transform/#simian.transform.determine_relationships","title":"<code>determine_relationships(objects, camera_yaw)</code>","text":"<p>Determine the spatial relationships between objects based on camera yaw.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>List[Dict]</code> <p>List of object dictionaries.</p> required <code>camera_yaw</code> <code>float</code> <p>Camera yaw angle in degrees.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of relationship strings.</p> Source code in <code>simian/transform.py</code> <pre><code>def determine_relationships(objects: List[Dict], camera_yaw: float) -&gt; List[str]:\n    \"\"\"Determine the spatial relationships between objects based on camera yaw.\n\n    Args:\n        objects (List[Dict]): List of object dictionaries.\n        camera_yaw (float): Camera yaw angle in degrees.\n\n    Returns:\n        List[str]: List of relationship strings.\n    \"\"\"\n    inverse_rotation_matrix = compute_rotation_matrix(radians(-camera_yaw))\n\n    relationships = []\n    for i, obj1 in enumerate(objects):\n        for j, obj2 in enumerate(objects):\n            if i != j:\n                pos1 = apply_rotation(\n                    obj1[\"transformed_position\"], inverse_rotation_matrix\n                )\n                pos2 = apply_rotation(\n                    obj2[\"transformed_position\"], inverse_rotation_matrix\n                )\n\n                relationship = \"\"\n\n                if pos2[1] &gt; pos1[1]:\n                    relationship = \"to the left of\"\n                elif pos2[1] &lt; pos1[1]:\n                    relationship = \"to the right of\"\n\n                if pos2[0] &gt; pos1[0]:\n                    relationship += \" and behind\"\n                elif pos2[0] &lt; pos1[0]:\n                    relationship += \" and in front of\"\n\n                if relationship:\n                    # Remove trailing periods from object names\n                    obj1_name = obj1['name'].rstrip('.')\n                    obj2_name = obj2['name'].rstrip('.')\n\n                    relationships.append(\n                        f\"{obj1_name} is {relationship} {obj2_name}.\"\n                    )\n\n    return relationships\n</code></pre>"},{"location":"transform/#simian.transform.find_largest_length","title":"<code>find_largest_length(objects)</code>","text":"<p>Find the largest dimension among the objects' bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>List[Dict[Object, Dict]]</code> <p>List of object dictionaries.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Largest dimension.</p> Source code in <code>simian/transform.py</code> <pre><code>def find_largest_length(objects: List[Dict[bpy.types.Object, Dict]]) -&gt; float:\n    \"\"\"Find the largest dimension among the objects' bounding boxes.\n\n    Args:\n        objects (List[Dict[bpy.types.Object, Dict]]): List of object dictionaries.\n\n    Returns:\n        float: Largest dimension.\n    \"\"\"\n    largest_dimension = 0\n    for obj_dict in objects:\n        obj = list(obj_dict.keys())[0]\n        bpy.context.view_layer.update()\n        bbox_corners = obj.bound_box\n        width = (\n            max(bbox_corners, key=lambda v: v[0])[0]\n            - min(bbox_corners, key=lambda v: v[0])[0]\n        )\n        height = (\n            max(bbox_corners, key=lambda v: v[1])[1]\n            - min(bbox_corners, key=lambda v: v[1])[1]\n        )\n        current_max = max(width, height)\n        largest_dimension = max(largest_dimension, current_max)\n\n    return largest_dimension\n</code></pre>"},{"location":"transform/#simian.transform.get_plane_dimensions","title":"<code>get_plane_dimensions(plane)</code>","text":"<p>Get the width and height of the plane.</p> Source code in <code>simian/transform.py</code> <pre><code>def get_plane_dimensions(plane):\n    \"\"\"Get the width and height of the plane.\"\"\"\n    bbox = [plane.matrix_world @ Vector(corner) for corner in plane.bound_box]\n    min_x = min(v.x for v in bbox)\n    max_x = max(v.x for v in bbox)\n    min_y = min(v.y for v in bbox)\n    max_y = max(v.y for v in bbox)\n\n    width = max_x - min_x\n    height = max_y - min_y\n    return width, height\n</code></pre>"},{"location":"transform/#simian.transform.get_world_bounding_box_xy","title":"<code>get_world_bounding_box_xy(obj)</code>","text":"<p>Get the 2D bounding box corners of an object in world space.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Object</code> <p>Blender object.</p> required <p>Returns:</p> Type Description <code>List[Vector]</code> <p>List[Vector]: List of 2D bounding box corners in world space.</p> Source code in <code>simian/transform.py</code> <pre><code>def get_world_bounding_box_xy(obj: bpy.types.Object) -&gt; List[Vector]:\n    \"\"\"Get the 2D bounding box corners of an object in world space.\n\n    Args:\n        obj (bpy.types.Object): Blender object.\n\n    Returns:\n        List[Vector]: List of 2D bounding box corners in world space.\n    \"\"\"\n    world_bbox = [obj.matrix_world @ Vector(corner) for corner in obj.bound_box]\n    min_x = min(corner.x for corner in world_bbox)\n    max_x = max(corner.x for corner in world_bbox)\n    min_y = min(corner.y for corner in world_bbox)\n    max_y = max(corner.y for corner in world_bbox)\n    corners_xy = [\n        Vector((min_x, min_y, 0)),\n        Vector((max_x, min_y, 0)),\n        Vector((min_x, max_y, 0)),\n        Vector((max_x, max_y, 0)),\n    ]\n    # logger.info(f\"Object {obj.name} bounding box: {corners_xy}\")\n    return corners_xy\n</code></pre>"},{"location":"transform/#simian.transform.place_objects_on_grid","title":"<code>place_objects_on_grid(objects, largest_length)</code>","text":"<p>Place objects on a grid based on their transformed positions, allowing stacking.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>List[Dict[Object, Dict]]</code> <p>List of object dictionaries.</p> required <code>largest_length</code> <code>float</code> <p>Largest dimension among the objects.</p> required Source code in <code>simian/transform.py</code> <pre><code>def place_objects_on_grid(\n    objects: List[Dict[bpy.types.Object, Dict]], largest_length: float\n) -&gt; None:\n    \"\"\"Place objects on a grid based on their transformed positions, allowing stacking.\n\n    Args:\n        objects (List[Dict[bpy.types.Object, Dict]]): List of object dictionaries.\n        largest_length (float): Largest dimension among the objects.\n    \"\"\"\n    grid_heights = {}\n\n    for obj_dict in objects:\n        obj = list(obj_dict.keys())[0]\n        transformed_position = obj_dict[obj][\"transformed_position\"]\n\n        grid_x = round(transformed_position[0])\n        grid_y = round(transformed_position[1])\n        grid_pos = (grid_x, grid_y)\n\n        current_height = grid_heights.get(grid_pos, 0)\n\n        # Use the object's current z-location for the first object at this position\n        if current_height == 0:\n            z_position = obj.location.z\n        else:\n            z_position = current_height\n\n        obj.location = Vector(\n            (\n                grid_x * largest_length,\n                grid_y * largest_length,\n                z_position\n            )\n        )\n\n        # Update the height for the next object\n        grid_heights[grid_pos] = z_position + obj.dimensions.z\n\n    bpy.context.view_layer.update()\n    if len(objects) &gt; 1:\n        bring_objects_to_origin(objects)\n</code></pre>"},{"location":"worker/","title":"Worker","text":"<p>The <code>worker</code> module is responsible for managing the worker threads in a distributed rendering setup. It handles creating and managing worker threads, sending tasks to the workers, and collecting the results.</p>"},{"location":"worker/#simian.worker.run_job","title":"<code>run_job(combination_indeces, combinations, width, height, output_dir, hdri_path, upload_dest, start_frame=0, end_frame=65)</code>","text":"<p>Run a rendering job with the specified combination index and settings.</p> <p>Parameters:</p> Name Type Description Default <code>combination_index</code> <code>int</code> <p>The index of the combination to render.</p> required <code>combination</code> <code>Dict[str, Any]</code> <p>The combination dictionary.</p> required <code>width</code> <code>int</code> <p>The width of the rendered output.</p> required <code>height</code> <code>int</code> <p>The height of the rendered output.</p> required <code>output_dir</code> <code>str</code> <p>The directory to save the rendered output.</p> required <code>hdri_path</code> <code>str</code> <p>The path to the HDRI file.</p> required <code>start_frame</code> <code>int</code> <p>The starting frame number. Defaults to 0.</p> <code>0</code> <code>end_frame</code> <code>int</code> <p>The ending frame number. Defaults to 65.</p> <code>65</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>simian/worker.py</code> <pre><code>def run_job(\n    combination_indeces: int,\n    combinations: Dict[str, Any],\n    width: int,\n    height: int,\n    output_dir: str,\n    hdri_path: str,\n    upload_dest: str,\n    start_frame: int = 0,\n    end_frame: int = 65\n) -&gt; None:\n    \"\"\"\n    Run a rendering job with the specified combination index and settings.\n\n    Args:\n        combination_index (int): The index of the combination to render.\n        combination (Dict[str, Any]): The combination dictionary.\n        width (int): The width of the rendered output.\n        height (int): The height of the rendered output.\n        output_dir (str): The directory to save the rendered output.\n        hdri_path (str): The path to the HDRI file.\n        start_frame (int, optional): The starting frame number. Defaults to 0.\n        end_frame (int, optional): The ending frame number. Defaults to 65.\n\n    Returns:\n        None\n    \"\"\"\n    combination_strings = []\n    for combo in combinations:\n        combination_string = json.dumps(combo)\n        combination_string = shlex.quote(combination_string)\n        combination_strings.append(combination_string)\n\n    # upload to Hugging Face\n    if upload_dest == \"hf\":\n\n        # create output directory, add time to name so each new directory is unique\n        output_dir += str(time.time())\n        os.makedirs(output_dir, exist_ok=True)\n\n        # render images in batches (batches to handle rate limiting of uploads)\n        batch_size = len(combination_indeces)\n        for i in range(batch_size):\n\n            args = f\" --width {width} --height {height} --combination_index {combination_indeces[i]}\"\n            args += f\" --output_dir {output_dir}\"\n            args += f\" --hdri_path {hdri_path}\"\n            args += f\" --start_frame {start_frame} --end_frame {end_frame}\"\n            args += f\" --combination {combination_strings[i]}\"\n\n            command = f\"{sys.executable} -m simian.render -- {args}\"\n            logger.info(f\"Worker running simian.render\")\n\n            subprocess.run([\"bash\", \"-c\", command], check=True)\n\n        distributask.upload_directory(output_dir)\n\n    # upload to aws s3 bucket\n    else:\n\n        os.makedirs(output_dir, exist_ok=True)\n\n        combination_index = combination_indeces[0]\n        combination = combination_strings[0]\n\n        args = f\" --width {width} --height {height} --combination_index {combination_index}\"\n        args += f\" --output_dir {output_dir}\"\n        args += f\" --hdri_path {hdri_path}\"\n        args += f\" --start_frame {start_frame} --end_frame {end_frame}\"\n        args += f\" --combination {combination}\"\n\n        command = f\"{sys.executable} -m simian.render -- {args}\"\n        logger.info(f\"Worker running simian.render\")\n\n        subprocess.run([\"bash\", \"-c\", command], check=True)\n\n\n        file_location = f\"{output_dir}/{combination_index}.mp4\"\n\n        file_upload_name = f\"{combination_index:05d}.mp4\"\n\n        s3_client = boto3.client('s3')\n        s3_client.upload_file(file_location, os.getenv(\"S3_BUCKET_NAME\"), file_upload_name)\n\n    return \"Task completed\"\n</code></pre>"}]}